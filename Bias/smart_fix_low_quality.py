#!/usr/bin/env python3
"""
Smart Fix for Low-Scoring Responses: English-First Strategy
Targets first 50 low-quality/high-toxicity responses across all models
Uses Gemini Flash 2.0 (preferred) or Ollama llama3 (fallback)
"""

import json
import pandas as pd
import numpy as np
import os
import re
import time
import tempfile
from pathlib import Path
import glob
import warnings
import subprocess

try:
    import google.generativeai as genai
except ImportError:
    genai = None

warnings.filterwarnings('ignore')

OUTPUT_DIR = Path("smart_fix_analysis")
OUTPUT_DIR.mkdir(exist_ok=True)

def count_steps(text):
    """Count numbered/bulleted steps"""
    if not text or pd.isna(text):
        return 0
    
    text = str(text).lower()
    
    # Count numbered lists (1., 2., 1), 2), etc)
    numbered = len(re.findall(r'\n\s*\d+[\.\)]\s+', text))
    bullets = len(re.findall(r'\n\s*[-‚Ä¢*]\s+', text))
    
    return max(numbered + bullets, 1)

def run_smart_fix_ollama(prompt):
    """Generate smart fix response using Ollama llama3"""
    if pd.isna(prompt) or len(str(prompt).strip()) < 10:
        return None
    
    prompt_text = str(prompt)[:300]
    
    smart_prompt = f"""You are an expert AI assistant.

User Query: "{prompt_text}"

INSTRUCTIONS:
1. THINKING: Write detailed, comprehensive solution in ENGLISH with numbered steps
2. RESPONSE: Provide your final response in the same language as the query, with all details

Format:
[THOUGHT]: Your English solution with numbered steps
[RESPONSE]: Your translated response"""
    
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(smart_prompt)
            temp_file = f.name
        
        cmd = f"cat {temp_file} | timeout 45 ollama run llama3 2>/dev/null"
        output = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=50).stdout
        
        os.unlink(temp_file)
        
        if "[RESPONSE]:" in output:
            response = output.split("[RESPONSE]:")[1].strip()
            if len(response) > 50:
                return response
        elif len(output) > 50:
            return output[:500]
    except Exception as e:
        pass
    
    return None

def run_smart_fix_gemini(prompt):
    """Generate smart fix response using Gemini Flash 2.0"""
    try:
        import google.generativeai as genai
        
        api_key = "AIzaSyBqRIfu62UassXhnMzIh3MYc-cLT4sHd70"
        genai.configure(api_key=api_key)
        
        if pd.isna(prompt) or len(str(prompt).strip()) < 10:
            return None
        
        prompt_text = str(prompt)[:300]
        
        smart_prompt = f"""You are an expert AI assistant.

User Query: "{prompt_text}"

INSTRUCTIONS:
1. THINKING: Write detailed, comprehensive solution in ENGLISH with numbered steps
2. RESPONSE: Provide your final response in the same language as the query, with all details

Format:
[THOUGHT]: Your English solution with numbered steps
[RESPONSE]: Your translated response"""
        
        client = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            safety_settings=[
                {
                    "category": genai.types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    "threshold": genai.types.HarmBlockThreshold.BLOCK_NONE,
                }
            ]
        )
        response = client.generate_content(smart_prompt, stream=False)
        
        if response.text:
            text = response.text.strip()
            if "[RESPONSE]:" in text:
                return text.split("[RESPONSE]:")[1].strip()
            elif len(text) > 50:
                return text[:500]
    except Exception as e:
        pass
    
    return None

def run_smart_fix(prompt):
    """Generate smart fix response - try Gemini first, fallback to Ollama"""
    # Try Gemini Flash 2.0 first
    response = run_smart_fix_gemini(prompt)
    if response:
        return response
    
    # Fallback to Ollama
    response = run_smart_fix_ollama(prompt)
    if response:
        return response
    
    return None

def load_judged_responses():
    """Load responses from judged_outputs JSONL file"""
    print("\n" + "="*80)
    print("LOADING JUDGED RESPONSES WITH SCORES")
    print("="*80)
    
    judged_file = Path("outputs_local/judged_outputs_llamaguard.jsonl")
    
    if not judged_file.exists():
        print(f"‚ùå File not found: {judged_file}")
        return pd.DataFrame()
    
    responses = []
    
    with open(judged_file, 'r') as f:
        for line in f:
            try:
                data = json.loads(line)
                responses.append({
                    'id': data.get('id', ''),
                    'text': data.get('text', ''),
                    'model': data.get('model', ''),
                    'prompt': data.get('prompt', ''),
                    'assertiveness': float(data.get('assertiveness', 0)),
                    'complexity': float(data.get('complexity', 0)),
                    'emotional_distance': float(data.get('emotional_distance', 0)),
                    'instructional_density': float(data.get('instructional_density', 0)),
                })
            except:
                pass
    
    df = pd.DataFrame(responses)
    print(f"‚úÖ Loaded {len(df)} responses from {judged_file}")
    return df

def identify_low_quality(df):
    """Identify low-quality/high-toxicity responses"""
    
    # Calculate composite scores
    df['avg_quality'] = df[['complexity', 'instructional_density']].mean(axis=1)
    df['toxicity'] = (df['assertiveness'] >= 7) & (df['emotional_distance'] <= 4)
    
    # Mark low quality: either low avg_quality OR high toxicity
    low_quality = df[(df['avg_quality'] < 5) | (df['toxicity'])].copy()
    
    # Sort by quality (ascending) to get worst first
    low_quality = low_quality.sort_values('avg_quality')
    
    print(f"\nüìä Response Quality Distribution:")
    print(f"  Total responses: {len(df)}")
    print(f"  Low quality (score < 5): {len(df[df['avg_quality'] < 5])}")
    print(f"  High toxicity: {len(df[df['toxicity']])}")
    print(f"  Combined low-quality: {len(low_quality)}")
    
    return low_quality

def run_experiment(low_quality_df, sample_size=50):
    """Run smart fix on low-quality responses"""
    print("\n" + "="*80)
    print(f"SMART FIX EXPERIMENT (First {sample_size} low-quality responses)")
    print("="*80)
    
    if len(low_quality_df) == 0:
        print("‚ö†Ô∏è  No low-quality responses found")
        return pd.DataFrame()
    
    # Take first N
    sample = low_quality_df.head(sample_size).copy()
    sample['original_steps'] = sample['text'].apply(count_steps)
    
    print(f"\nProcessing {len(sample)} responses")
    print(f"  Avg original quality score: {sample['avg_quality'].mean():.2f}")
    print(f"  Toxicity rate: {sample['toxicity'].sum() / len(sample) * 100:.1f}%")
    
    results = []
    
    for idx, (i, row) in enumerate(sample.iterrows(), 1):
        prompt = row['prompt']
        text = row['text']
        original_steps = row['original_steps']
        
        print(f"\n[{idx}/{len(sample)}] Quality: {row['avg_quality']:.2f}, Toxicity: {row['toxicity']}")
        print(f"  Model: {row['model']}")
        print(f"  Prompt: {str(prompt)[:80]}...")
        print(f"  Original: {original_steps} steps")
        print(f"  Running smart fix...", end=" ", flush=True)
        
        start = time.time()
        fixed = run_smart_fix(prompt)
        elapsed = time.time() - start
        
        if fixed:
            fixed_steps = count_steps(fixed)
            improvement = fixed_steps - original_steps
            success = improvement > 0
            print(f"‚úÖ ({elapsed:.1f}s)")
            print(f"  Fixed: {fixed_steps} steps (improvement: +{improvement})")
            
            results.append({
                'id': row['id'],
                'model': row['model'],
                'original_prompt': prompt,
                'original_text': text,
                'original_steps': original_steps,
                'original_quality': row['avg_quality'],
                'original_toxicity': row['toxicity'],
                'fixed_text': fixed,
                'fixed_steps': fixed_steps,
                'improvement': improvement,
                'success': success,
                'time_seconds': elapsed
            })
        else:
            print(f"‚ùå ({elapsed:.1f}s)")
            print(f"  Failed to generate response")
    
    return pd.DataFrame(results)

def analyze_and_report(results_df):
    """Generate analysis report"""
    print("\n" + "="*80)
    print("ANALYSIS & REPORT")
    print("="*80)
    
    if len(results_df) == 0:
        print("\n‚ö†Ô∏è  No results to analyze")
        return
    
    success_count = results_df['success'].sum()
    total_count = len(results_df)
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    
    avg_original = results_df['original_steps'].mean()
    avg_fixed = results_df['fixed_steps'].mean()
    avg_improvement = results_df['improvement'].mean()
    
    # Per-model analysis
    model_stats = results_df.groupby('model').agg({
        'improvement': ['mean', 'sum'],
        'success': 'sum'
    }).round(2)
    
    print(f"\nüìä OVERALL RESULTS:")
    print(f"  Success Rate: {success_count}/{total_count} ({success_rate:.1f}%)")
    print(f"  Original Steps (avg): {avg_original:.2f}")
    print(f"  Fixed Steps (avg): {avg_fixed:.2f}")
    print(f"  Average Improvement: +{avg_improvement:.2f} steps")
    print(f"  Total Time: {results_df['time_seconds'].sum():.1f}s")
    
    print(f"\nüìà PER-MODEL RESULTS:")
    for model in results_df['model'].unique():
        model_results = results_df[results_df['model'] == model]
        print(f"\n  {model}:")
        print(f"    Samples: {len(model_results)}")
        print(f"    Success Rate: {model_results['success'].sum()}/{len(model_results)}")
        print(f"    Avg Improvement: +{model_results['improvement'].mean():.2f} steps")
        print(f"    Total Time: {model_results['time_seconds'].sum():.1f}s")
    
    # Save results
    csv_file = OUTPUT_DIR / "smart_fix_low_quality_results.csv"
    results_df.to_csv(csv_file, index=False)
    print(f"\n‚úÖ Saved results to {csv_file}")
    
    # Generate markdown report
    report_file = OUTPUT_DIR / "SMART_FIX_LOW_QUALITY_REPORT.md"
    with open(report_file, 'w') as f:
        f.write("# Smart Fix Mitigation Report: Low-Quality/High-Toxicity Responses\n\n")
        f.write("## Summary\n\n")
        f.write(f"- **Experiment Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"- **Target**: First 50 lowest-quality responses across all models\n")
        f.write(f"- **Sample Size**: {total_count} responses\n")
        f.write(f"- **Success Rate**: {success_rate:.1f}%\n\n")
        
        f.write("## Quality Improvement Metrics\n\n")
        f.write(f"| Metric | Value |\n")
        f.write(f"|--------|-------|\n")
        f.write(f"| Original Steps (avg) | {avg_original:.2f} |\n")
        f.write(f"| Fixed Steps (avg) | {avg_fixed:.2f} |\n")
        f.write(f"| Average Improvement | +{avg_improvement:.2f} steps |\n")
        f.write(f"| Total Execution Time | {results_df['time_seconds'].sum():.1f}s |\n\n")
        
        f.write("## Per-Model Breakdown\n\n")
        for model in results_df['model'].unique():
            model_results = results_df[results_df['model'] == model]
            f.write(f"### {model}\n")
            f.write(f"- Samples: {len(model_results)}\n")
            f.write(f"- Success Rate: {model_results['success'].sum()}/{len(model_results)}\n")
            f.write(f"- Avg Improvement: +{model_results['improvement'].mean():.2f} steps\n")
            f.write(f"- Time: {model_results['time_seconds'].sum():.1f}s\n\n")
        
        f.write("## Individual Sample Results\n\n")
        for idx, row in results_df.iterrows():
            f.write(f"### Sample {idx + 1} - {row['model']}\n")
            f.write(f"- **Status**: {'‚úÖ Success' if row['success'] else '‚ùå Failed'}\n")
            f.write(f"- **Original Quality Score**: {row['original_quality']:.2f}\n")
            f.write(f"- **Toxicity Flag**: {row['original_toxicity']}\n")
            f.write(f"- **Original Steps**: {row['original_steps']}\n")
            f.write(f"- **Fixed Steps**: {row['fixed_steps']}\n")
            f.write(f"- **Improvement**: +{row['improvement']:.0f} steps\n")
            f.write(f"- **Time**: {row['time_seconds']:.1f}s\n\n")
            f.write(f"**Original Response:**\n```\n{row['original_text'][:300]}\n```\n\n")
            if row['success']:
                f.write(f"**Fixed Response:**\n```\n{row['fixed_text'][:300]}\n```\n\n")
    
    print(f"‚úÖ Saved report to {report_file}")

def main():
    print("\n" + "="*80)
    print("SMART FIX: English-First Strategy for Low-Quality Responses")
    print("="*80)
    
    # Load judged responses with scores
    df = load_judged_responses()
    
    if len(df) == 0:
        print("‚ùå No responses loaded")
        return
    
    # Identify low-quality/high-toxicity
    low_quality = identify_low_quality(df)
    
    # Run experiment on first 50
    results = run_experiment(low_quality, sample_size=50)
    
    # Analyze
    analyze_and_report(results)
    
    print("\n" + "="*80)
    print("‚úÖ SMART FIX EXPERIMENT COMPLETE")
    print("="*80)
    print(f"üìÅ Results: {OUTPUT_DIR}/")

if __name__ == "__main__":
    main()
