{"model": "qwen25_7b", "model_id": "Qwen/Qwen2.5-7B-Instruct", "target_layer_index": 14, "num_hidden_layers": 28, "hidden_size": 3584, "intermediate_size": 18944, "num_attention_heads": 28, "rope_theta": 1000000.0, "max_position_embeddings": 32768, "vocab_size": 152064, "model_type": "qwen2", "model_dtype": "torch.float16"}
{"model": "qwen25_7b", "model_id": "Qwen/Qwen2.5-7B-Instruct", "target_layer_index": 14, "num_hidden_layers": 28, "hidden_size": 3584, "intermediate_size": 18944, "num_attention_heads": 28, "rope_theta": 1000000.0, "max_position_embeddings": 32768, "vocab_size": 152064, "model_type": "qwen2", "model_dtype": "torch.float16"}
{"model": "qwen2_7b", "model_id": "Qwen/Qwen2-7B-Instruct", "target_layer_index": 14, "num_hidden_layers": 28, "hidden_size": 3584, "intermediate_size": 18944, "num_attention_heads": 28, "rope_theta": 1000000.0, "max_position_embeddings": 32768, "vocab_size": 152064, "model_type": "qwen2", "model_dtype": "torch.float16"}
{"model": "zephyr_7b", "model_id": "HuggingFaceH4/zephyr-7b-beta", "target_layer_index": 16, "num_hidden_layers": 32, "hidden_size": 4096, "intermediate_size": 14336, "num_attention_heads": 32, "rope_theta": 10000.0, "max_position_embeddings": 32768, "vocab_size": 32000, "model_type": "mistral", "model_dtype": "torch.float16"}
{"model": "openhermes_mistral", "model_id": "teknium/OpenHermes-2.5-Mistral-7B", "target_layer_index": 16, "num_hidden_layers": 32, "hidden_size": 4096, "intermediate_size": 14336, "num_attention_heads": 32, "rope_theta": 10000.0, "max_position_embeddings": 32768, "vocab_size": 32002, "model_type": "mistral", "model_dtype": "torch.float16"}
{"model": "nous_hermes_mistral", "model_id": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO", "target_layer_index": 16, "num_hidden_layers": 32, "hidden_size": 4096, "intermediate_size": 14336, "num_attention_heads": 32, "rope_theta": 10000.0, "max_position_embeddings": 32768, "vocab_size": 32002, "model_type": "mistral", "model_dtype": "torch.float16"}
{"model": "phi35_mini", "model_id": "microsoft/Phi-3.5-mini-instruct", "target_layer_index": 16, "num_hidden_layers": 32, "hidden_size": 3072, "intermediate_size": 8192, "num_attention_heads": 32, "rope_theta": 10000.0, "max_position_embeddings": 131072, "vocab_size": 32064, "model_type": "phi3", "model_dtype": "torch.float16"}
