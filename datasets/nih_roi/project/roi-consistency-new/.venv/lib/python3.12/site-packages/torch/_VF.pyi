# @generated by tools/pyi/gen_pyi.py from torch/_C/_VariableFunctions.pyi.in
# mypy: disable-error-code="type-arg"
# mypy: allow-untyped-defs
# ruff: noqa: F401,PYI054

from collections.abc import Sequence
from types import EllipsisType
from typing import Any, Callable, Literal, overload, TypeVar

import torch
from torch import (
    contiguous_format,
    Generator,
    inf,
    memory_format,
    strided,
    SymInt,
    Tensor,
)
from torch._prims_common import DeviceLikeType
from torch.types import (
    _bool,
    _complex,
    _device,
    _dtype,
    _float,
    _int,
    _layout,
    _qscheme,
    _size,
    Device,
    Number,
)

__all__ = [
    "__and__",
    "__lshift__",
    "__or__",
    "__rshift__",
    "__xor__",
    "_adaptive_avg_pool2d",
    "_adaptive_avg_pool3d",
    "_add_batch_dim",
    "_add_relu",
    "_add_relu_",
    "_addmm_activation",
    "_aminmax",
    "_amp_foreach_non_finite_check_and_unscale_",
    "_amp_update_scale_",
    "_assert_async",
    "_assert_scalar",
    "_assert_tensor_metadata",
    "_batch_norm_impl_index",
    "_cast_Byte",
    "_cast_Char",
    "_cast_Double",
    "_cast_Float",
    "_cast_Half",
    "_cast_Int",
    "_cast_Long",
    "_cast_Short",
    "_choose_qparams_per_tensor",
    "_chunk_cat",
    "_coalesce",
    "_compute_linear_combination",
    "_conj",
    "_conj_copy",
    "_conj_physical",
    "_convert_indices_from_coo_to_csr",
    "_convert_indices_from_csr_to_coo",
    "_convert_weight_to_int4pack",
    "_convert_weight_to_int4pack_for_cpu",
    "_convolution",
    "_convolution_mode",
    "_copy_from",
    "_copy_from_and_resize",
    "_cslt_compress",
    "_cslt_sparse_mm",
    "_cslt_sparse_mm_search",
    "_ctc_loss",
    "_cudnn_ctc_loss",
    "_cudnn_init_dropout_state",
    "_cudnn_rnn",
    "_cudnn_rnn_flatten_weight",
    "_cufft_clear_plan_cache",
    "_cufft_get_plan_cache_max_size",
    "_cufft_get_plan_cache_size",
    "_cufft_set_plan_cache_max_size",
    "_cummax_helper",
    "_cummin_helper",
    "_debug_has_internal_overlap",
    "_dim_arange",
    "_dirichlet_grad",
    "_disable_functionalization",
    "_dyn_quant_matmul_4bit",
    "_dyn_quant_pack_4bit_weight",
    "_efficientzerotensor",
    "_embedding_bag",
    "_embedding_bag_forward_only",
    "_empty_affine_quantized",
    "_empty_per_channel_affine_quantized",
    "_enable_functionalization",
    "_euclidean_dist",
    "_fake_quantize_learnable_per_channel_affine",
    "_fake_quantize_learnable_per_tensor_affine",
    "_fake_quantize_per_tensor_affine_cachemask_tensor_qparams",
    "_fft_c2c",
    "_fft_c2r",
    "_fft_r2c",
    "_fill_mem_eff_dropout_mask_",
    "_foobar",
    "_foreach_abs",
    "_foreach_abs_",
    "_foreach_acos",
    "_foreach_acos_",
    "_foreach_add",
    "_foreach_add_",
    "_foreach_addcdiv",
    "_foreach_addcdiv_",
    "_foreach_addcmul",
    "_foreach_addcmul_",
    "_foreach_asin",
    "_foreach_asin_",
    "_foreach_atan",
    "_foreach_atan_",
    "_foreach_ceil",
    "_foreach_ceil_",
    "_foreach_clamp_max",
    "_foreach_clamp_max_",
    "_foreach_clamp_min",
    "_foreach_clamp_min_",
    "_foreach_copy_",
    "_foreach_cos",
    "_foreach_cos_",
    "_foreach_cosh",
    "_foreach_cosh_",
    "_foreach_div",
    "_foreach_div_",
    "_foreach_erf",
    "_foreach_erf_",
    "_foreach_erfc",
    "_foreach_erfc_",
    "_foreach_exp",
    "_foreach_exp_",
    "_foreach_expm1",
    "_foreach_expm1_",
    "_foreach_floor",
    "_foreach_floor_",
    "_foreach_frac",
    "_foreach_frac_",
    "_foreach_lerp",
    "_foreach_lerp_",
    "_foreach_lgamma",
    "_foreach_lgamma_",
    "_foreach_log",
    "_foreach_log10",
    "_foreach_log10_",
    "_foreach_log1p",
    "_foreach_log1p_",
    "_foreach_log2",
    "_foreach_log2_",
    "_foreach_log_",
    "_foreach_max",
    "_foreach_maximum",
    "_foreach_maximum_",
    "_foreach_minimum",
    "_foreach_minimum_",
    "_foreach_mul",
    "_foreach_mul_",
    "_foreach_neg",
    "_foreach_neg_",
    "_foreach_norm",
    "_foreach_pow",
    "_foreach_pow_",
    "_foreach_reciprocal",
    "_foreach_reciprocal_",
    "_foreach_round",
    "_foreach_round_",
    "_foreach_rsqrt",
    "_foreach_rsqrt_",
    "_foreach_sigmoid",
    "_foreach_sigmoid_",
    "_foreach_sign",
    "_foreach_sign_",
    "_foreach_sin",
    "_foreach_sin_",
    "_foreach_sinh",
    "_foreach_sinh_",
    "_foreach_sqrt",
    "_foreach_sqrt_",
    "_foreach_sub",
    "_foreach_sub_",
    "_foreach_tan",
    "_foreach_tan_",
    "_foreach_tanh",
    "_foreach_tanh_",
    "_foreach_trunc",
    "_foreach_trunc_",
    "_foreach_zero_",
    "_from_functional_tensor",
    "_functional_assert_async",
    "_functional_assert_scalar",
    "_functional_sym_constrain_range",
    "_functional_sym_constrain_range_for_size",
    "_functionalize_apply_view_metas",
    "_functionalize_are_all_mutations_hidden_from_autograd",
    "_functionalize_are_all_mutations_under_no_grad_or_inference_mode",
    "_functionalize_commit_update",
    "_functionalize_has_metadata_mutation",
    "_functionalize_inductor_storage_resized_counter",
    "_functionalize_is_symbolic",
    "_functionalize_mark_mutation_hidden_from_autograd",
    "_functionalize_mark_storage_changed",
    "_functionalize_mutation_counter",
    "_functionalize_replace",
    "_functionalize_storage_changed_counter",
    "_functionalize_sync",
    "_functionalize_unsafe_set",
    "_functionalize_was_inductor_storage_resized",
    "_functionalize_was_storage_changed",
    "_fused_adagrad_",
    "_fused_adam_",
    "_fused_adamw_",
    "_fused_dropout",
    "_fused_moving_avg_obs_fq_helper",
    "_fused_rms_norm",
    "_fused_sdp_choice",
    "_fused_sgd_",
    "_fw_primal_copy",
    "_grid_sampler_2d_cpu_fallback",
    "_grouped_mm",
    "_has_compatible_shallow_copy_type",
    "_histogramdd_bin_edges",
    "_histogramdd_from_bin_cts",
    "_histogramdd_from_bin_tensors",
    "_index_put_impl_",
    "_indices_copy",
    "_int_mm",
    "_is_all_true",
    "_is_any_true",
    "_is_functional_tensor",
    "_is_functional_tensor_base",
    "_is_zerotensor",
    "_lazy_clone",
    "_linalg_check_errors",
    "_linalg_det",
    "_linalg_eigh",
    "_linalg_slogdet",
    "_linalg_solve_ex",
    "_linalg_svd",
    "_log_softmax",
    "_log_softmax_backward_data",
    "_logcumsumexp",
    "_lstm_mps",
    "_lu_with_info",
    "_make_dep_token",
    "_make_dual",
    "_make_dual_copy",
    "_make_per_channel_quantized_tensor",
    "_make_per_tensor_quantized_tensor",
    "_masked_scale",
    "_masked_softmax",
    "_mixed_dtypes_linear",
    "_mkldnn_reshape",
    "_mkldnn_transpose",
    "_mkldnn_transpose_",
    "_mps_convolution",
    "_mps_convolution_transpose",
    "_native_batch_norm_legit",
    "_native_batch_norm_legit_no_training",
    "_native_multi_head_attention",
    "_neg_view",
    "_neg_view_copy",
    "_nested_compute_contiguous_strides_offsets",
    "_nested_from_padded",
    "_nested_from_padded_and_nested_example",
    "_nested_from_padded_tensor",
    "_nested_get_jagged_dummy",
    "_nested_get_lengths",
    "_nested_get_max_seqlen",
    "_nested_get_min_seqlen",
    "_nested_get_offsets",
    "_nested_get_ragged_idx",
    "_nested_get_values",
    "_nested_get_values_copy",
    "_nested_tensor_from_mask",
    "_nested_tensor_from_mask_left_aligned",
    "_nested_tensor_from_tensor_list",
    "_nested_tensor_softmax_with_shape",
    "_nested_view_from_buffer",
    "_nested_view_from_buffer_copy",
    "_nested_view_from_jagged",
    "_nested_view_from_jagged_copy",
    "_nnpack_available",
    "_nnpack_spatial_convolution",
    "_pack_padded_sequence",
    "_pad_packed_sequence",
    "_pin_memory",
    "_prelu_kernel",
    "_print",
    "_propagate_xla_data",
    "_remove_batch_dim",
    "_reshape_alias_copy",
    "_reshape_from_tensor",
    "_resize_output_",
    "_rowwise_prune",
    "_safe_softmax",
    "_sample_dirichlet",
    "_saturate_weight_to_fp16",
    "_scaled_dot_product_attention_math",
    "_scaled_dot_product_attention_math_for_mps",
    "_scaled_dot_product_cudnn_attention",
    "_scaled_dot_product_efficient_attention",
    "_scaled_dot_product_flash_attention",
    "_scaled_dot_product_flash_attention_for_cpu",
    "_scaled_grouped_mm",
    "_scaled_mm",
    "_shape_as_tensor",
    "_sobol_engine_draw",
    "_sobol_engine_ff_",
    "_sobol_engine_initialize_state_",
    "_sobol_engine_scramble_",
    "_softmax",
    "_softmax_backward_data",
    "_sparse_broadcast_to",
    "_sparse_broadcast_to_copy",
    "_sparse_csr_prod",
    "_sparse_csr_sum",
    "_sparse_log_softmax_backward_data",
    "_sparse_semi_structured_addmm",
    "_sparse_semi_structured_apply",
    "_sparse_semi_structured_apply_dense",
    "_sparse_semi_structured_linear",
    "_sparse_semi_structured_mm",
    "_sparse_semi_structured_tile",
    "_sparse_softmax_backward_data",
    "_sparse_sparse_matmul",
    "_sparse_sum",
    "_stack",
    "_standard_gamma",
    "_standard_gamma_grad",
    "_sync",
    "_test_autograd_multiple_dispatch",
    "_test_autograd_multiple_dispatch_view",
    "_test_autograd_multiple_dispatch_view_copy",
    "_test_check_tensor",
    "_test_functorch_fallback",
    "_test_parallel_materialize",
    "_test_serialization_subcmul",
    "_to_cpu",
    "_to_functional_tensor",
    "_to_sparse_semi_structured",
    "_transform_bias_rescale_qkv",
    "_transformer_encoder_layer_fwd",
    "_trilinear",
    "_triton_multi_head_attention",
    "_triton_scaled_dot_attention",
    "_unique",
    "_unique2",
    "_unpack_dual",
    "_unsafe_index",
    "_unsafe_index_put",
    "_unsafe_masked_index",
    "_unsafe_masked_index_put_accumulate",
    "_use_cudnn_ctc_loss",
    "_use_cudnn_rnn_flatten_weight",
    "_validate_compressed_sparse_indices",
    "_validate_sparse_bsc_tensor_args",
    "_validate_sparse_bsr_tensor_args",
    "_validate_sparse_compressed_tensor_args",
    "_validate_sparse_coo_tensor_args",
    "_validate_sparse_csc_tensor_args",
    "_validate_sparse_csr_tensor_args",
    "_values_copy",
    "_weight_int4pack_mm",
    "_weight_int4pack_mm_for_cpu",
    "_weight_int4pack_mm_with_scales_and_zeros",
    "_weight_int8pack_mm",
    "_weight_norm",
    "_weight_norm_interface",
    "_wrapped_linear_prepack",
    "_wrapped_quantized_linear_prepacked",
    "abs",
    "abs_",
    "absolute",
    "acos",
    "acos_",
    "acosh",
    "acosh_",
    "adaptive_avg_pool1d",
    "adaptive_max_pool1d",
    "add",
    "addbmm",
    "addcdiv",
    "addcmul",
    "addmm",
    "addmv",
    "addmv_",
    "addr",
    "adjoint",
    "affine_grid_generator",
    "alias_copy",
    "all",
    "allclose",
    "alpha_dropout",
    "alpha_dropout_",
    "amax",
    "amin",
    "aminmax",
    "angle",
    "any",
    "arange",
    "arccos",
    "arccos_",
    "arccosh",
    "arccosh_",
    "arcsin",
    "arcsin_",
    "arcsinh",
    "arcsinh_",
    "arctan",
    "arctan2",
    "arctan_",
    "arctanh",
    "arctanh_",
    "argmax",
    "argmin",
    "argsort",
    "argwhere",
    "as_strided",
    "as_strided_",
    "as_strided_copy",
    "as_strided_scatter",
    "as_tensor",
    "asarray",
    "asin",
    "asin_",
    "asinh",
    "asinh_",
    "atan",
    "atan2",
    "atan_",
    "atanh",
    "atanh_",
    "avg_pool1d",
    "baddbmm",
    "bartlett_window",
    "batch_norm",
    "batch_norm_backward_elemt",
    "batch_norm_backward_reduce",
    "batch_norm_elemt",
    "batch_norm_gather_stats",
    "batch_norm_gather_stats_with_counts",
    "batch_norm_stats",
    "batch_norm_update_stats",
    "bernoulli",
    "bilinear",
    "binary_cross_entropy_with_logits",
    "bincount",
    "binomial",
    "bitwise_and",
    "bitwise_left_shift",
    "bitwise_not",
    "bitwise_or",
    "bitwise_right_shift",
    "bitwise_xor",
    "blackman_window",
    "bmm",
    "broadcast_to",
    "bucketize",
    "can_cast",
    "cat",
    "ccol_indices_copy",
    "ceil",
    "ceil_",
    "celu",
    "celu_",
    "channel_shuffle",
    "cholesky",
    "cholesky_inverse",
    "cholesky_solve",
    "choose_qparams_optimized",
    "chunk",
    "clamp",
    "clamp_",
    "clamp_max",
    "clamp_max_",
    "clamp_min",
    "clamp_min_",
    "clip",
    "clip_",
    "clone",
    "col_indices_copy",
    "column_stack",
    "combinations",
    "complex",
    "concat",
    "concatenate",
    "conj",
    "conj_physical",
    "conj_physical_",
    "constant_pad_nd",
    "conv1d",
    "conv2d",
    "conv3d",
    "conv_tbc",
    "conv_transpose1d",
    "conv_transpose2d",
    "conv_transpose3d",
    "convolution",
    "copysign",
    "corrcoef",
    "cos",
    "cos_",
    "cosh",
    "cosh_",
    "cosine_embedding_loss",
    "cosine_similarity",
    "count_nonzero",
    "cov",
    "cross",
    "crow_indices_copy",
    "ctc_loss",
    "cudnn_affine_grid_generator",
    "cudnn_batch_norm",
    "cudnn_convolution",
    "cudnn_convolution_add_relu",
    "cudnn_convolution_relu",
    "cudnn_convolution_transpose",
    "cudnn_grid_sampler",
    "cudnn_is_acceptable",
    "cummax",
    "cummin",
    "cumprod",
    "cumsum",
    "cumulative_trapezoid",
    "deg2rad",
    "deg2rad_",
    "dequantize",
    "det",
    "detach",
    "detach_",
    "detach_copy",
    "diag",
    "diag_embed",
    "diagflat",
    "diagonal",
    "diagonal_copy",
    "diagonal_scatter",
    "diff",
    "digamma",
    "dist",
    "div",
    "divide",
    "dot",
    "dropout",
    "dropout_",
    "dsmm",
    "dsplit",
    "dstack",
    "embedding",
    "embedding_bag",
    "embedding_renorm_",
    "empty",
    "empty_like",
    "empty_permuted",
    "empty_quantized",
    "empty_strided",
    "eq",
    "equal",
    "erf",
    "erf_",
    "erfc",
    "erfc_",
    "erfinv",
    "exp",
    "exp2",
    "exp2_",
    "exp_",
    "expand_copy",
    "expm1",
    "expm1_",
    "eye",
    "fake_quantize_per_channel_affine",
    "fake_quantize_per_tensor_affine",
    "fbgemm_linear_fp16_weight",
    "fbgemm_linear_fp16_weight_fp32_activation",
    "fbgemm_linear_int8_weight",
    "fbgemm_linear_int8_weight_fp32_activation",
    "fbgemm_linear_quantize_weight",
    "fbgemm_pack_gemm_matrix_fp16",
    "fbgemm_pack_quantized_matrix",
    "feature_alpha_dropout",
    "feature_alpha_dropout_",
    "feature_dropout",
    "feature_dropout_",
    "fill",
    "fill_",
    "fix",
    "fix_",
    "flatten",
    "flip",
    "fliplr",
    "flipud",
    "float_power",
    "floor",
    "floor_",
    "floor_divide",
    "fmax",
    "fmin",
    "fmod",
    "frac",
    "frac_",
    "frexp",
    "frobenius_norm",
    "from_file",
    "from_numpy",
    "frombuffer",
    "full",
    "full_like",
    "fused_moving_avg_obs_fake_quant",
    "gather",
    "gcd",
    "gcd_",
    "ge",
    "geqrf",
    "ger",
    "get_default_dtype",
    "get_num_interop_threads",
    "get_num_threads",
    "gradient",
    "greater",
    "greater_equal",
    "grid_sampler",
    "grid_sampler_2d",
    "grid_sampler_3d",
    "group_norm",
    "gru",
    "gru_cell",
    "gt",
    "hamming_window",
    "hann_window",
    "hardshrink",
    "hash_tensor",
    "heaviside",
    "hinge_embedding_loss",
    "histc",
    "histogram",
    "histogramdd",
    "hsmm",
    "hsplit",
    "hspmm",
    "hstack",
    "hypot",
    "i0",
    "i0_",
    "igamma",
    "igammac",
    "imag",
    "index_add",
    "index_copy",
    "index_fill",
    "index_put",
    "index_put_",
    "index_reduce",
    "index_select",
    "indices_copy",
    "init_num_threads",
    "inner",
    "instance_norm",
    "int_repr",
    "inverse",
    "is_complex",
    "is_conj",
    "is_distributed",
    "is_floating_point",
    "is_grad_enabled",
    "is_inference",
    "is_inference_mode_enabled",
    "is_neg",
    "is_nonzero",
    "is_same_size",
    "is_signed",
    "is_vulkan_available",
    "isclose",
    "isfinite",
    "isin",
    "isinf",
    "isnan",
    "isneginf",
    "isposinf",
    "isreal",
    "istft",
    "kaiser_window",
    "kl_div",
    "kron",
    "kthvalue",
    "layer_norm",
    "lcm",
    "lcm_",
    "ldexp",
    "ldexp_",
    "le",
    "lerp",
    "less",
    "less_equal",
    "lgamma",
    "linspace",
    "log",
    "log10",
    "log10_",
    "log1p",
    "log1p_",
    "log2",
    "log2_",
    "log_",
    "log_softmax",
    "logaddexp",
    "logaddexp2",
    "logcumsumexp",
    "logdet",
    "logical_and",
    "logical_not",
    "logical_or",
    "logical_xor",
    "logit",
    "logit_",
    "logspace",
    "logsumexp",
    "lstm",
    "lstm_cell",
    "lt",
    "lu_solve",
    "lu_unpack",
    "margin_ranking_loss",
    "masked_fill",
    "masked_scatter",
    "masked_select",
    "matmul",
    "matrix_exp",
    "matrix_power",
    "max",
    "max_pool1d",
    "max_pool1d_with_indices",
    "max_pool2d",
    "max_pool3d",
    "maximum",
    "mean",
    "median",
    "min",
    "minimum",
    "miopen_batch_norm",
    "miopen_convolution",
    "miopen_convolution_add_relu",
    "miopen_convolution_relu",
    "miopen_convolution_transpose",
    "miopen_depthwise_convolution",
    "miopen_rnn",
    "mkldnn_adaptive_avg_pool2d",
    "mkldnn_convolution",
    "mkldnn_linear_backward_weights",
    "mkldnn_max_pool2d",
    "mkldnn_max_pool3d",
    "mkldnn_rnn_layer",
    "mm",
    "mode",
    "moveaxis",
    "movedim",
    "msort",
    "mul",
    "multinomial",
    "multiply",
    "mv",
    "mvlgamma",
    "nan_to_num",
    "nan_to_num_",
    "nanmean",
    "nanmedian",
    "nanquantile",
    "nansum",
    "narrow",
    "narrow_copy",
    "native_batch_norm",
    "native_channel_shuffle",
    "native_dropout",
    "native_group_norm",
    "native_layer_norm",
    "native_norm",
    "ne",
    "neg",
    "neg_",
    "negative",
    "negative_",
    "nextafter",
    "nonzero",
    "nonzero_static",
    "norm_except_dim",
    "normal",
    "not_equal",
    "nuclear_norm",
    "numel",
    "ones",
    "ones_like",
    "orgqr",
    "ormqr",
    "outer",
    "pairwise_distance",
    "pdist",
    "permute",
    "permute_copy",
    "pinverse",
    "pixel_shuffle",
    "pixel_unshuffle",
    "poisson",
    "poisson_nll_loss",
    "polar",
    "polygamma",
    "positive",
    "pow",
    "prelu",
    "prod",
    "promote_types",
    "put",
    "q_per_channel_axis",
    "q_per_channel_scales",
    "q_per_channel_zero_points",
    "q_scale",
    "q_zero_point",
    "qr",
    "quantile",
    "quantize_per_channel",
    "quantize_per_tensor",
    "quantize_per_tensor_dynamic",
    "quantized_batch_norm",
    "quantized_gru_cell",
    "quantized_lstm_cell",
    "quantized_max_pool1d",
    "quantized_max_pool2d",
    "quantized_max_pool3d",
    "quantized_rnn_relu_cell",
    "quantized_rnn_tanh_cell",
    "rad2deg",
    "rad2deg_",
    "rand",
    "rand_like",
    "randint",
    "randint_like",
    "randn",
    "randn_like",
    "randperm",
    "range",
    "ravel",
    "real",
    "reciprocal",
    "reciprocal_",
    "relu",
    "relu_",
    "remainder",
    "renorm",
    "repeat_interleave",
    "reshape",
    "resize_as_",
    "resize_as_sparse_",
    "resolve_conj",
    "resolve_neg",
    "result_type",
    "rms_norm",
    "rnn_relu",
    "rnn_relu_cell",
    "rnn_tanh",
    "rnn_tanh_cell",
    "roll",
    "rot90",
    "round",
    "round_",
    "row_indices_copy",
    "row_stack",
    "rrelu",
    "rrelu_",
    "rsqrt",
    "rsqrt_",
    "rsub",
    "saddmm",
    "scalar_tensor",
    "scatter",
    "scatter_add",
    "scatter_reduce",
    "searchsorted",
    "segment_reduce",
    "select",
    "select_copy",
    "select_scatter",
    "selu",
    "selu_",
    "set_flush_denormal",
    "set_num_interop_threads",
    "set_num_threads",
    "sgn",
    "sigmoid",
    "sigmoid_",
    "sign",
    "signbit",
    "sin",
    "sin_",
    "sinc",
    "sinc_",
    "sinh",
    "sinh_",
    "slice_copy",
    "slice_inverse",
    "slice_scatter",
    "slogdet",
    "smm",
    "softmax",
    "sort",
    "sparse_bsc_tensor",
    "sparse_bsr_tensor",
    "sparse_compressed_tensor",
    "sparse_coo_tensor",
    "sparse_csc_tensor",
    "sparse_csr_tensor",
    "split_copy",
    "split_with_sizes",
    "split_with_sizes_copy",
    "spmm",
    "sqrt",
    "sqrt_",
    "square",
    "square_",
    "squeeze",
    "squeeze_copy",
    "sspaddmm",
    "stack",
    "std",
    "std_mean",
    "sub",
    "subtract",
    "sum",
    "svd",
    "swapaxes",
    "swapdims",
    "sym_constrain_range",
    "sym_constrain_range_for_size",
    "t",
    "t_copy",
    "take",
    "take_along_dim",
    "tan",
    "tan_",
    "tanh",
    "tanh_",
    "tensor",
    "tensor_split",
    "threshold",
    "threshold_",
    "tile",
    "topk",
    "trace",
    "transpose",
    "transpose_copy",
    "trapezoid",
    "trapz",
    "triangular_solve",
    "tril",
    "tril_indices",
    "triplet_margin_loss",
    "triu",
    "triu_indices",
    "true_divide",
    "trunc",
    "trunc_",
    "unbind",
    "unbind_copy",
    "unflatten",
    "unfold_copy",
    "unique_dim",
    "unsafe_chunk",
    "unsafe_split",
    "unsafe_split_with_sizes",
    "unsqueeze",
    "unsqueeze_copy",
    "values_copy",
    "vander",
    "var",
    "var_mean",
    "vdot",
    "view_as_complex",
    "view_as_complex_copy",
    "view_as_real",
    "view_as_real_copy",
    "view_copy",
    "vsplit",
    "vstack",
    "where",
    "xlogy",
    "xlogy_",
    "zero_",
    "zeros",
    "zeros_like",
]

@overload
def __and__(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def __and__(input: Tensor, other: Number | _complex) -> Tensor: ...
@overload
def __lshift__(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def __lshift__(input: Tensor, other: Number | _complex) -> Tensor: ...
@overload
def __or__(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def __or__(input: Tensor, other: Number | _complex) -> Tensor: ...
@overload
def __rshift__(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def __rshift__(input: Tensor, other: Number | _complex) -> Tensor: ...
@overload
def __xor__(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def __xor__(input: Tensor, other: Number | _complex) -> Tensor: ...
def _adaptive_avg_pool2d(
    input: Tensor,
    output_size: _int | SymInt | Sequence[_int | SymInt],
) -> Tensor: ...
def _adaptive_avg_pool3d(
    input: Tensor,
    output_size: _int | SymInt | Sequence[_int | SymInt],
) -> Tensor: ...
def _add_batch_dim(input: Tensor, batch_dim: _int, level: _int) -> Tensor: ...
@overload
def _add_relu(
    input: Tensor,
    other: Tensor,
    *,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor: ...
@overload
def _add_relu(
    input: Tensor,
    other: Number | _complex,
    alpha: Number | _complex = 1,
) -> Tensor: ...
@overload
def _add_relu_(
    input: Tensor,
    other: Tensor,
    *,
    alpha: Number | _complex = 1,
) -> Tensor: ...
@overload
def _add_relu_(
    input: Tensor,
    other: Number | _complex,
    alpha: Number | _complex = 1,
) -> Tensor: ...
def _addmm_activation(
    input: Tensor,
    mat1: Tensor,
    mat2: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    use_gelu: _bool = False,
    out: Tensor | None = None,
) -> Tensor: ...
@overload
def _aminmax(input: Tensor) -> tuple[Tensor, Tensor]: ...
@overload
def _aminmax(
    input: Tensor,
    dim: _int,
    keepdim: _bool = False,
) -> tuple[Tensor, Tensor]: ...
def _amp_foreach_non_finite_check_and_unscale_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    found_inf: Tensor,
    inv_scale: Tensor,
) -> None: ...
def _amp_update_scale_(
    input: Tensor,
    growth_tracker: Tensor,
    found_inf: Tensor,
    scale_growth_factor: _float,
    scale_backoff_factor: _float,
    growth_interval: _int,
) -> Tensor: ...
@overload
def _assert_async(input: Tensor) -> None:
    r"""
    _assert_async(tensor) -> void

    Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
    this is equivalent to ``assert tensor`` or ``assert tensor.is_nonzero()``; for
    CUDA tensors, we DO NOT synchronize and you may only find out the assertion
    failed at a later CUDA kernel launch.  Asynchronous assertion can be helpful for
    testing invariants in CUDA tensors without giving up performance.  This function
    is NOT intended to be used for regular error checking, as it will trash your CUDA
    context if the assert fails (forcing you to restart your PyTorch process.)

    Args:
        tensor (Tensor): a one element tensor to test to see if it is nonzero.  Zero
            elements (including False for boolean tensors) cause an assertion failure
            to be raised.
    """

@overload
def _assert_async(input: Tensor, assert_msg: str) -> None:
    r"""
    _assert_async(tensor) -> void

    Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
    this is equivalent to ``assert tensor`` or ``assert tensor.is_nonzero()``; for
    CUDA tensors, we DO NOT synchronize and you may only find out the assertion
    failed at a later CUDA kernel launch.  Asynchronous assertion can be helpful for
    testing invariants in CUDA tensors without giving up performance.  This function
    is NOT intended to be used for regular error checking, as it will trash your CUDA
    context if the assert fails (forcing you to restart your PyTorch process.)

    Args:
        tensor (Tensor): a one element tensor to test to see if it is nonzero.  Zero
            elements (including False for boolean tensors) cause an assertion failure
            to be raised.
    """

def _assert_scalar(self: Number | _complex, assert_msg: str) -> None: ...
def _assert_tensor_metadata(
    a: Tensor,
    size: Sequence[_int | SymInt] | None = None,
    stride: Sequence[_int | SymInt] | None = None,
    dtype: _dtype | None = None,
    *,
    device: DeviceLikeType | None = None,
    layout: _layout | None = None,
) -> None: ...
def _batch_norm_impl_index(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    running_mean: Tensor | None,
    running_var: Tensor | None,
    training: _bool,
    momentum: _float,
    eps: _float,
    cudnn_enabled: _bool,
) -> tuple[Tensor, Tensor, Tensor, Tensor, _int]: ...
def _cast_Byte(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Char(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Double(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Float(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Half(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Int(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Long(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _cast_Short(input: Tensor, non_blocking: _bool = False) -> Tensor: ...
def _choose_qparams_per_tensor(
    input: Tensor,
    reduce_range: _bool = False,
) -> tuple[_float, _int]: ...
def _chunk_cat(
    tensors: tuple[Tensor, ...] | list[Tensor] | None,
    dim: _int,
    num_chunks: _int,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _coalesce(input: Tensor) -> Tensor: ...
def _compute_linear_combination(
    input: Tensor,
    coefficients: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _conj(input: Tensor) -> Tensor: ...
def _conj_copy(input: Tensor, *, out: Tensor | None = None) -> Tensor: ...
def _conj_physical(input: Tensor) -> Tensor: ...
def _convert_indices_from_coo_to_csr(
    input: Tensor,
    size: _int,
    *,
    out_int32: _bool = False,
    out: Tensor | None = None,
) -> Tensor: ...
def _convert_indices_from_csr_to_coo(
    crow_indices: Tensor,
    col_indices: Tensor,
    *,
    out_int32: _bool = False,
    transpose: _bool = False,
    out: Tensor | None = None,
) -> Tensor: ...
def _convert_weight_to_int4pack(input: Tensor, innerKTiles: _int) -> Tensor: ...
def _convert_weight_to_int4pack_for_cpu(
    input: Tensor,
    innerKTiles: _int,
) -> Tensor: ...
@overload
def _convolution(
    input: Tensor,
    weight: Tensor,
    bias: Tensor | None,
    stride: Sequence[_int | SymInt],
    padding: Sequence[_int | SymInt],
    dilation: Sequence[_int | SymInt],
    transposed: _bool,
    output_padding: _size,
    groups: _int | SymInt,
    benchmark: _bool,
    deterministic: _bool,
    cudnn_enabled: _bool,
) -> Tensor: ...
@overload
def _convolution(
    input: Tensor,
    weight: Tensor,
    bias: Tensor | None,
    stride: Sequence[_int | SymInt],
    padding: Sequence[_int | SymInt],
    dilation: Sequence[_int | SymInt],
    transposed: _bool,
    output_padding: Sequence[_int | SymInt],
    groups: _int | SymInt,
    benchmark: _bool,
    deterministic: _bool,
    cudnn_enabled: _bool,
    allow_tf32: _bool,
) -> Tensor: ...
def _convolution_mode(
    input: Tensor,
    weight: Tensor,
    bias: Tensor | None,
    stride: Sequence[_int | SymInt],
    padding: str,
    dilation: Sequence[_int | SymInt],
    groups: _int | SymInt,
) -> Tensor: ...
def _copy_from(
    input: Tensor,
    dst: Tensor,
    non_blocking: _bool = False,
) -> Tensor: ...
def _copy_from_and_resize(input: Tensor, dst: Tensor) -> Tensor: ...
def _cslt_compress(input: Tensor) -> Tensor: ...
def _cslt_sparse_mm(
    compressed_A: Tensor,
    dense_B: Tensor,
    bias: Tensor | None = None,
    alpha: Tensor | None = None,
    out_dtype: _dtype | None = None,
    transpose_result: _bool = False,
    alg_id: _int = 0,
    split_k: _int = 1,
    split_k_mode: _int = -1,
) -> Tensor: ...
def _cslt_sparse_mm_search(
    compressed_A: Tensor,
    dense_B: Tensor,
    bias: Tensor | None = None,
    alpha: Tensor | None = None,
    out_dtype: _dtype | None = None,
    transpose_result: _bool = False,
) -> _int: ...
@overload
def _ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: _size,
    target_lengths: _size,
    blank: _int = 0,
    zero_infinity: _bool = False,
) -> tuple[Tensor, Tensor]: ...
@overload
def _ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: Tensor,
    target_lengths: Tensor,
    blank: _int = 0,
    zero_infinity: _bool = False,
) -> tuple[Tensor, Tensor]: ...
@overload
def _cudnn_ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: _size,
    target_lengths: _size,
    blank: _int,
    deterministic: _bool,
    zero_infinity: _bool,
) -> tuple[Tensor, Tensor]: ...
@overload
def _cudnn_ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: Tensor,
    target_lengths: Tensor,
    blank: _int,
    deterministic: _bool,
    zero_infinity: _bool,
) -> tuple[Tensor, Tensor]: ...
def _cudnn_init_dropout_state(
    dropout: _float,
    train: _bool,
    dropout_seed: _int,
    *,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
def _cudnn_rnn(
    input: Tensor,
    weight: tuple[Tensor, ...] | list[Tensor] | None,
    weight_stride0: _int,
    weight_buf: Tensor | None,
    hx: Tensor,
    cx: Tensor | None,
    mode: _int,
    hidden_size: _int | SymInt,
    proj_size: _int | SymInt,
    num_layers: _int,
    batch_first: _bool,
    dropout: _float,
    train: _bool,
    bidirectional: _bool,
    batch_sizes: Sequence[_int | SymInt],
    dropout_state: Tensor | None,
) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _cudnn_rnn_flatten_weight(
    weight_arr: tuple[Tensor, ...] | list[Tensor] | None,
    weight_stride0: _int,
    input_size: _int | SymInt,
    mode: _int,
    hidden_size: _int | SymInt,
    proj_size: _int | SymInt,
    num_layers: _int,
    batch_first: _bool,
    bidirectional: _bool,
) -> Tensor: ...
def _cufft_clear_plan_cache(device_index: _int) -> None: ...
def _cufft_get_plan_cache_max_size(device_index: _int) -> _int: ...
def _cufft_get_plan_cache_size(device_index: _int) -> _int: ...
def _cufft_set_plan_cache_max_size(
    device_index: _int,
    max_size: _int,
) -> None: ...
def _cummax_helper(
    input: Tensor,
    values: Tensor,
    indices: Tensor,
    dim: _int,
) -> None: ...
def _cummin_helper(
    input: Tensor,
    values: Tensor,
    indices: Tensor,
    dim: _int,
) -> None: ...
def _debug_has_internal_overlap(input: Tensor) -> _int: ...
def _dim_arange(like: Tensor, dim: _int) -> Tensor: ...
def _dirichlet_grad(x: Tensor, alpha: Tensor, total: Tensor) -> Tensor: ...
def _disable_functionalization(): ...
def _dyn_quant_matmul_4bit(
    inp: Tensor,
    packed_weights: Tensor,
    block_size: _int,
    in_features: _int,
    out_features: _int,
) -> Tensor: ...
def _dyn_quant_pack_4bit_weight(
    weights: Tensor,
    scales_zeros: Tensor,
    bias: Tensor | None,
    block_size: _int,
    in_features: _int,
    out_features: _int,
) -> Tensor: ...
@overload
def _efficientzerotensor(
    size: Sequence[_int | SymInt],
    *,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
@overload
def _efficientzerotensor(
    *size: _int | SymInt,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
def _embedding_bag(
    weight: Tensor,
    indices: Tensor,
    offsets: Tensor,
    scale_grad_by_freq: _bool = False,
    mode: _int = 0,
    sparse: _bool = False,
    per_sample_weights: Tensor | None = None,
    include_last_offset: _bool = False,
    padding_idx: _int = -1,
) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
def _embedding_bag_forward_only(
    weight: Tensor,
    indices: Tensor,
    offsets: Tensor,
    scale_grad_by_freq: _bool = False,
    mode: _int = 0,
    sparse: _bool = False,
    per_sample_weights: Tensor | None = None,
    include_last_offset: _bool = False,
    padding_idx: _int = -1,
) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
@overload
def _empty_affine_quantized(
    size: Sequence[_int | SymInt],
    *,
    scale: _float = 1,
    zero_point: _int = 0,
    memory_format: memory_format | None = contiguous_format,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
@overload
def _empty_affine_quantized(
    *size: _int | SymInt,
    scale: _float = 1,
    zero_point: _int = 0,
    memory_format: memory_format | None = contiguous_format,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
@overload
def _empty_per_channel_affine_quantized(
    size: Sequence[_int | SymInt],
    *,
    scales: Tensor,
    zero_points: Tensor,
    axis: _int,
    memory_format: memory_format | None = contiguous_format,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
@overload
def _empty_per_channel_affine_quantized(
    *size: _int | SymInt,
    scales: Tensor,
    zero_points: Tensor,
    axis: _int,
    memory_format: memory_format | None = contiguous_format,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
def _enable_functionalization(*, reapply_views: _bool = False) -> None: ...
def _euclidean_dist(x1: Tensor, x2: Tensor) -> Tensor: ...
def _fake_quantize_learnable_per_channel_affine(
    input: Tensor,
    scale: Tensor,
    zero_point: Tensor,
    axis: _int,
    quant_min: _int,
    quant_max: _int,
    grad_factor: _float = 1.0,
) -> Tensor: ...
def _fake_quantize_learnable_per_tensor_affine(
    input: Tensor,
    scale: Tensor,
    zero_point: Tensor,
    quant_min: _int,
    quant_max: _int,
    grad_factor: _float = 1.0,
) -> Tensor: ...
def _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(
    input: Tensor,
    scale: Tensor,
    zero_point: Tensor,
    fake_quant_enabled: Tensor,
    quant_min: _int,
    quant_max: _int,
) -> torch.return_types._fake_quantize_per_tensor_affine_cachemask_tensor_qparams:  # fmt: skip
    ...
def _fft_c2c(
    input: Tensor,
    dim: Sequence[_int | SymInt],
    normalization: _int,
    forward: _bool,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _fft_c2r(
    input: Tensor,
    dim: _size,
    normalization: _int,
    last_dim_size: _int | SymInt,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _fft_r2c(
    input: Tensor,
    dim: _size,
    normalization: _int,
    onesided: _bool,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _fill_mem_eff_dropout_mask_(
    input: Tensor,
    dropout_p: _float,
    seed: _int,
    offset: _int,
) -> Tensor: ...
def _foobar(
    input: Tensor,
    arg1: _bool = True,
    arg2: _bool = True,
    *,
    arg3: _bool = True,
) -> Tensor: ...
def _foreach_abs(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_abs(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.abs` to each Tensor of the input list.
    """

def _foreach_abs_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_abs_(self: List[Tensor]) -> None

    Apply :func:`torch.abs` to each Tensor of the input list.
    """

def _foreach_acos(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_acos(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.acos` to each Tensor of the input list.
    """

def _foreach_acos_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_acos_(self: List[Tensor]) -> None

    Apply :func:`torch.acos` to each Tensor of the input list.
    """

@overload
def _foreach_add(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_add(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    alpha: Number | _complex = 1,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_add(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
    *,
    alpha: Number | _complex = 1,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_add(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_add_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_add_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    alpha: Number | _complex = 1,
) -> None: ...
@overload
def _foreach_add_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
    *,
    alpha: Number | _complex = 1,
) -> None: ...
@overload
def _foreach_add_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_addcdiv(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcdiv(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Tensor,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcdiv(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    value: Number | _complex = 1,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcdiv_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_addcdiv_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Tensor,
) -> None: ...
@overload
def _foreach_addcdiv_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    value: Number | _complex = 1,
) -> None: ...
@overload
def _foreach_addcmul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcmul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Tensor,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcmul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    value: Number | _complex = 1,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_addcmul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_addcmul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Tensor,
) -> None: ...
@overload
def _foreach_addcmul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensor1: tuple[Tensor, ...] | list[Tensor] | None,
    tensor2: tuple[Tensor, ...] | list[Tensor] | None,
    value: Number | _complex = 1,
) -> None: ...
def _foreach_asin(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_asin(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.asin` to each Tensor of the input list.
    """

def _foreach_asin_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_asin_(self: List[Tensor]) -> None

    Apply :func:`torch.asin` to each Tensor of the input list.
    """

def _foreach_atan(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_atan(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.atan` to each Tensor of the input list.
    """

def _foreach_atan_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_atan_(self: List[Tensor]) -> None

    Apply :func:`torch.atan` to each Tensor of the input list.
    """

def _foreach_ceil(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_ceil(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.ceil` to each Tensor of the input list.
    """

def _foreach_ceil_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_ceil_(self: List[Tensor]) -> None

    Apply :func:`torch.ceil` to each Tensor of the input list.
    """

@overload
def _foreach_clamp_max(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_max(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_max(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_max_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_clamp_max_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_clamp_max_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
@overload
def _foreach_clamp_min(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_min(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_min(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_clamp_min_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_clamp_min_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_clamp_min_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
def _foreach_copy_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    src: tuple[Tensor, ...] | list[Tensor] | None,
    non_blocking: _bool = False,
) -> None: ...
def _foreach_cos(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_cos(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.cos` to each Tensor of the input list.
    """

def _foreach_cos_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_cos_(self: List[Tensor]) -> None

    Apply :func:`torch.cos` to each Tensor of the input list.
    """

def _foreach_cosh(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_cosh(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.cosh` to each Tensor of the input list.
    """

def _foreach_cosh_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_cosh_(self: List[Tensor]) -> None

    Apply :func:`torch.cosh` to each Tensor of the input list.
    """

@overload
def _foreach_div(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_div(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_div(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_div(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_div_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_div_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
) -> None: ...
@overload
def _foreach_div_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_div_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
def _foreach_erf(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_erf(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.erf` to each Tensor of the input list.
    """

def _foreach_erf_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_erf_(self: List[Tensor]) -> None

    Apply :func:`torch.erf` to each Tensor of the input list.
    """

def _foreach_erfc(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_erfc(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.erfc` to each Tensor of the input list.
    """

def _foreach_erfc_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_erfc_(self: List[Tensor]) -> None

    Apply :func:`torch.erfc` to each Tensor of the input list.
    """

def _foreach_exp(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_exp(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.exp` to each Tensor of the input list.
    """

def _foreach_exp_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_exp_(self: List[Tensor]) -> None

    Apply :func:`torch.exp` to each Tensor of the input list.
    """

def _foreach_expm1(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_expm1(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.expm1` to each Tensor of the input list.
    """

def _foreach_expm1_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_expm1_(self: List[Tensor]) -> None

    Apply :func:`torch.expm1` to each Tensor of the input list.
    """

def _foreach_floor(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_floor(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.floor` to each Tensor of the input list.
    """

def _foreach_floor_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_floor_(self: List[Tensor]) -> None

    Apply :func:`torch.floor` to each Tensor of the input list.
    """

def _foreach_frac(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_frac(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.frac` to each Tensor of the input list.
    """

def _foreach_frac_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_frac_(self: List[Tensor]) -> None

    Apply :func:`torch.frac` to each Tensor of the input list.
    """

@overload
def _foreach_lerp(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weight: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_lerp(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weight: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_lerp(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weights: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_lerp_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weight: Number | _complex,
) -> None: ...
@overload
def _foreach_lerp_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weight: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_lerp_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    tensors1: tuple[Tensor, ...] | list[Tensor] | None,
    weights: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
def _foreach_lgamma(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_lgamma(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.lgamma` to each Tensor of the input list.
    """

def _foreach_lgamma_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> None:
    r"""
    _foreach_lgamma_(self: List[Tensor]) -> None

    Apply :func:`torch.lgamma` to each Tensor of the input list.
    """

def _foreach_log(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_log(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.log` to each Tensor of the input list.
    """

def _foreach_log10(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_log10(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.log10` to each Tensor of the input list.
    """

def _foreach_log10_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_log10_(self: List[Tensor]) -> None

    Apply :func:`torch.log10` to each Tensor of the input list.
    """

def _foreach_log1p(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_log1p(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.log1p` to each Tensor of the input list.
    """

def _foreach_log1p_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_log1p_(self: List[Tensor]) -> None

    Apply :func:`torch.log1p` to each Tensor of the input list.
    """

def _foreach_log2(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_log2(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.log2` to each Tensor of the input list.
    """

def _foreach_log2_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_log2_(self: List[Tensor]) -> None

    Apply :func:`torch.log2` to each Tensor of the input list.
    """

def _foreach_log_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_log_(self: List[Tensor]) -> None

    Apply :func:`torch.log` to each Tensor of the input list.
    """

def _foreach_max(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_maximum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_maximum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_maximum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_maximum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_maximum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_maximum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
@overload
def _foreach_minimum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_minimum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_minimum(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_minimum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_minimum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_minimum_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
@overload
def _foreach_mul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_mul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_mul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_mul(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_mul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_mul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: Tensor,
) -> None: ...
@overload
def _foreach_mul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
@overload
def _foreach_mul_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
def _foreach_neg(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_neg(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.neg` to each Tensor of the input list.
    """

def _foreach_neg_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_neg_(self: List[Tensor]) -> None

    Apply :func:`torch.neg` to each Tensor of the input list.
    """

def _foreach_norm(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    ord: Number | _complex = 2,
    dtype: _dtype | None = None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_pow(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_pow(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_pow(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_pow(
    self: Number | _complex,
    exponent: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_pow_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_pow_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: Number | _complex,
) -> None: ...
@overload
def _foreach_pow_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    exponent: tuple[Tensor, ...] | list[Tensor] | None,
) -> None: ...
def _foreach_reciprocal(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_reciprocal(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.reciprocal` to each Tensor of the input list.
    """

def _foreach_reciprocal_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> None:
    r"""
    _foreach_reciprocal_(self: List[Tensor]) -> None

    Apply :func:`torch.reciprocal` to each Tensor of the input list.
    """

def _foreach_round(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_round(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.round` to each Tensor of the input list.
    """

def _foreach_round_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_round_(self: List[Tensor]) -> None

    Apply :func:`torch.round` to each Tensor of the input list.
    """

def _foreach_rsqrt(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
def _foreach_rsqrt_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None: ...
def _foreach_sigmoid(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_sigmoid(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.sigmoid` to each Tensor of the input list.
    """

def _foreach_sigmoid_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> None:
    r"""
    _foreach_sigmoid_(self: List[Tensor]) -> None

    Apply :func:`torch.sigmoid` to each Tensor of the input list.
    """

def _foreach_sign(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
def _foreach_sign_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None: ...
def _foreach_sin(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_sin(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.sin` to each Tensor of the input list.
    """

def _foreach_sin_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_sin_(self: List[Tensor]) -> None

    Apply :func:`torch.sin` to each Tensor of the input list.
    """

def _foreach_sinh(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_sinh(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.sinh` to each Tensor of the input list.
    """

def _foreach_sinh_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_sinh_(self: List[Tensor]) -> None

    Apply :func:`torch.sinh` to each Tensor of the input list.
    """

def _foreach_sqrt(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_sqrt(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.sqrt` to each Tensor of the input list.
    """

def _foreach_sqrt_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_sqrt_(self: List[Tensor]) -> None

    Apply :func:`torch.sqrt` to each Tensor of the input list.
    """

@overload
def _foreach_sub(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_sub(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    alpha: Number | _complex = 1,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_sub(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> tuple[Tensor, ...]: ...
@overload
def _foreach_sub_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalars: Sequence[Number | _complex],
) -> None: ...
@overload
def _foreach_sub_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    other: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    alpha: Number | _complex = 1,
) -> None: ...
@overload
def _foreach_sub_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    scalar: Number | _complex,
) -> None: ...
def _foreach_tan(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_tan(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.tan` to each Tensor of the input list.
    """

def _foreach_tan_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_tan_(self: List[Tensor]) -> None

    Apply :func:`torch.tan` to each Tensor of the input list.
    """

def _foreach_tanh(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_tanh(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.tanh` to each Tensor of the input list.
    """

def _foreach_tanh_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_tanh_(self: List[Tensor]) -> None

    Apply :func:`torch.tanh` to each Tensor of the input list.
    """

def _foreach_trunc(
    self: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]:
    r"""
    _foreach_trunc(self: List[Tensor]) -> List[Tensor]

    Apply :func:`torch.trunc` to each Tensor of the input list.
    """

def _foreach_trunc_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_trunc_(self: List[Tensor]) -> None

    Apply :func:`torch.trunc` to each Tensor of the input list.
    """

def _foreach_zero_(self: tuple[Tensor, ...] | list[Tensor] | None) -> None:
    r"""
    _foreach_zero_(self: List[Tensor]) -> None

    Apply :func:`torch.zero` to each Tensor of the input list.
    """

def _from_functional_tensor(t: Tensor) -> Tensor: ...
def _functional_assert_async(
    input: Tensor,
    assert_msg: str,
    dep_token: Tensor,
) -> Tensor: ...
def _functional_assert_scalar(
    self: Number | _complex,
    assert_msg: str,
    dep_token: Tensor,
) -> Tensor: ...
def _functional_sym_constrain_range(
    size: Number | _complex,
    min: _int | None,
    max: _int | None,
    dep_token: Tensor,
) -> Tensor: ...
def _functional_sym_constrain_range_for_size(
    size: Number | _complex,
    min: _int | None,
    max: _int | None,
    dep_token: Tensor,
) -> Tensor: ...
def _functionalize_apply_view_metas(tensor: Tensor, base: Tensor) -> Tensor: ...
def _functionalize_are_all_mutations_hidden_from_autograd(
    t: Tensor,
) -> _bool: ...
def _functionalize_are_all_mutations_under_no_grad_or_inference_mode(
    t: Tensor,
) -> _bool: ...
def _functionalize_commit_update(t: Tensor) -> None: ...
def _functionalize_has_metadata_mutation(tensor: Tensor) -> _bool: ...
def _functionalize_inductor_storage_resized_counter(t: Tensor) -> _int: ...
def _functionalize_is_symbolic(tensor: Tensor) -> _bool: ...
def _functionalize_mark_mutation_hidden_from_autograd(t: Tensor) -> None: ...
def _functionalize_mark_storage_changed(tensor: Tensor) -> _bool: ...
def _functionalize_mutation_counter(t: Tensor) -> _int: ...
def _functionalize_replace(self_: Tensor, other: Tensor) -> None: ...
def _functionalize_storage_changed_counter(t: Tensor) -> _int: ...
def _functionalize_sync(t: Tensor) -> None: ...
def _functionalize_unsafe_set(dst: Tensor, src: Tensor) -> None: ...
def _functionalize_was_inductor_storage_resized(t: Tensor) -> _bool: ...
def _functionalize_was_storage_changed(tensor: Tensor) -> _bool: ...
@overload
def _fused_adagrad_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    state_sums: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: Tensor,
    lr_decay: _float,
    weight_decay: _float,
    eps: _float,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_adagrad_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    state_sums: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: _float,
    lr_decay: _float,
    weight_decay: _float,
    eps: _float,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_adam_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avgs: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    max_exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: Tensor,
    beta1: _float,
    beta2: _float,
    weight_decay: _float,
    eps: _float,
    amsgrad: _bool,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_adam_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avgs: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    max_exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: _float,
    beta1: _float,
    beta2: _float,
    weight_decay: _float,
    eps: _float,
    amsgrad: _bool,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_adamw_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avgs: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    max_exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: Tensor,
    beta1: _float,
    beta2: _float,
    weight_decay: _float,
    eps: _float,
    amsgrad: _bool,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_adamw_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avgs: tuple[Tensor, ...] | list[Tensor] | None,
    exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    max_exp_avg_sqs: tuple[Tensor, ...] | list[Tensor] | None,
    state_steps: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    lr: _float,
    beta1: _float,
    beta2: _float,
    weight_decay: _float,
    eps: _float,
    amsgrad: _bool,
    maximize: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
def _fused_dropout(
    input: Tensor,
    p: _float,
    generator: Generator | None = None,
) -> tuple[Tensor, Tensor]: ...
def _fused_moving_avg_obs_fq_helper(
    input: Tensor,
    observer_on: Tensor,
    fake_quant_on: Tensor,
    running_min: Tensor,
    running_max: Tensor,
    scale: Tensor,
    zero_point: Tensor,
    averaging_const: _float,
    quant_min: _int,
    quant_max: _int,
    ch_axis: _int,
    per_row_fake_quant: _bool = False,
    symmetric_quant: _bool = False,
) -> torch.return_types._fused_moving_avg_obs_fq_helper: ...
def _fused_rms_norm(
    input: Tensor,
    normalized_shape: _size,
    weight: Tensor | None,
    eps: _float | None,
) -> tuple[Tensor, Tensor]: ...
def _fused_sdp_choice(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attn_mask: Tensor | None = None,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    *,
    scale: _float | None = None,
    enable_gqa: _bool = False,
) -> _int: ...
@overload
def _fused_sgd_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    momentum_buffer_list: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    weight_decay: _float,
    momentum: _float,
    lr: Tensor,
    dampening: _float,
    nesterov: _bool,
    maximize: _bool,
    is_first_step: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
@overload
def _fused_sgd_(
    self: tuple[Tensor, ...] | list[Tensor] | None,
    grads: tuple[Tensor, ...] | list[Tensor] | None,
    momentum_buffer_list: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    weight_decay: _float,
    momentum: _float,
    lr: _float,
    dampening: _float,
    nesterov: _bool,
    maximize: _bool,
    is_first_step: _bool,
    grad_scale: Tensor | None = None,
    found_inf: Tensor | None = None,
) -> None: ...
def _fw_primal_copy(
    input: Tensor,
    level: _int,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _grid_sampler_2d_cpu_fallback(
    input: Tensor,
    grid: Tensor,
    interpolation_mode: _int,
    padding_mode: _int,
    align_corners: _bool,
) -> Tensor: ...
def _grouped_mm(
    input: Tensor,
    mat2: Tensor,
    offs: Tensor | None = None,
    bias: Tensor | None = None,
    out_dtype: _dtype | None = None,
) -> Tensor: ...
def _has_compatible_shallow_copy_type(
    input: Tensor,
    from_: Tensor,
) -> _bool: ...
def _histogramdd_bin_edges(
    input: Tensor,
    bins: _size,
    *,
    range: Sequence[_float] | None = None,
    weight: Tensor | None = None,
    density: _bool = False,
) -> tuple[Tensor, ...]: ...
def _histogramdd_from_bin_cts(
    input: Tensor,
    bins: _size,
    *,
    range: Sequence[_float] | None = None,
    weight: Tensor | None = None,
    density: _bool = False,
) -> Tensor: ...
def _histogramdd_from_bin_tensors(
    input: Tensor,
    bins: tuple[Tensor, ...] | list[Tensor] | None,
    *,
    weight: Tensor | None = None,
    density: _bool = False,
) -> Tensor: ...
def _index_put_impl_(
    input: Tensor,
    indices: tuple[Tensor, ...] | list[Tensor] | None,
    values: Tensor,
    accumulate: _bool = False,
    unsafe: _bool = False,
) -> Tensor: ...
def _indices_copy(input: Tensor, *, out: Tensor | None = None) -> Tensor: ...
def _int_mm(
    input: Tensor,
    mat2: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _is_all_true(input: Tensor) -> Tensor: ...
def _is_any_true(input: Tensor) -> Tensor: ...
def _is_functional_tensor(t: Tensor) -> _bool: ...
def _is_functional_tensor_base(t: Tensor) -> _bool: ...
def _is_zerotensor(input: Tensor) -> _bool: ...
def _lazy_clone(input: Tensor) -> Tensor: ...
def _linalg_check_errors(
    info: Tensor,
    api_name: str,
    *,
    is_matrix: _bool,
) -> None: ...
def _linalg_det(
    A: Tensor,
    *,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types._linalg_det: ...
def _linalg_eigh(
    A: Tensor,
    UPLO: str = "L",
    compute_v: _bool = True,
    *,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types._linalg_eigh: ...
def _linalg_slogdet(
    A: Tensor,
    *,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types._linalg_slogdet: ...
def _linalg_solve_ex(
    A: Tensor,
    B: Tensor,
    *,
    left: _bool = True,
    check_errors: _bool = False,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types._linalg_solve_ex: ...
def _linalg_svd(
    A: Tensor,
    full_matrices: _bool = False,
    compute_uv: _bool = True,
    *,
    driver: str | None = None,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types._linalg_svd: ...
def _log_softmax(
    input: Tensor,
    dim: _int,
    half_to_float: _bool,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _log_softmax_backward_data(
    grad_output: Tensor,
    output: Tensor,
    dim: _int,
    input_dtype: _dtype,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _logcumsumexp(
    input: Tensor,
    dim: _int,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _lstm_mps(
    input: Tensor,
    hx: tuple[Tensor, ...] | list[Tensor] | None,
    params: tuple[Tensor, ...] | list[Tensor] | None,
    has_biases: _bool,
    num_layers: _int,
    dropout: _float,
    train: _bool,
    bidirectional: _bool,
    batch_first: _bool,
) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _lu_with_info(
    input: Tensor,
    pivot: _bool = True,
    check_errors: _bool = True,
) -> torch.return_types._lu_with_info: ...
def _make_dep_token(
    *,
    memory_format: memory_format | None = None,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor: ...
def _make_dual(primal: Tensor, tangent: Tensor, level: _int) -> Tensor: ...
def _make_dual_copy(
    primal: Tensor,
    tangent: Tensor,
    level: _int,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _make_per_channel_quantized_tensor(
    input: Tensor,
    scale: Tensor,
    zero_point: Tensor,
    axis: _int,
) -> Tensor: ...
def _make_per_tensor_quantized_tensor(
    input: Tensor,
    scale: _float,
    zero_point: _int,
) -> Tensor: ...
def _masked_scale(input: Tensor, mask: Tensor, scale: _float) -> Tensor: ...
def _masked_softmax(
    input: Tensor,
    mask: Tensor,
    dim: _int | None = None,
    mask_type: _int | None = None,
) -> Tensor: ...
def _mixed_dtypes_linear(
    input: Tensor,
    weight: Tensor,
    scale: Tensor,
    *,
    bias: Tensor | None = None,
    activation: str | None = None,
) -> Tensor: ...
def _mkldnn_reshape(input: Tensor, shape: _size) -> Tensor: ...
def _mkldnn_transpose(input: Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mkldnn_transpose_(input: Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mps_convolution(
    input: Tensor,
    weight: Tensor,
    bias: Tensor | None,
    padding: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    dilation: Sequence[_int | SymInt],
    groups: _int | SymInt,
) -> Tensor: ...
def _mps_convolution_transpose(
    input: Tensor,
    weight: Tensor,
    padding: Sequence[_int | SymInt],
    output_padding: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    dilation: Sequence[_int | SymInt],
    groups: _int | SymInt,
) -> Tensor: ...
@overload
def _native_batch_norm_legit(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    running_mean: Tensor,
    running_var: Tensor,
    training: _bool,
    momentum: _float,
    eps: _float,
    *,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> tuple[Tensor, Tensor, Tensor]: ...
@overload
def _native_batch_norm_legit(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    training: _bool,
    momentum: _float,
    eps: _float,
    *,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> tuple[Tensor, Tensor, Tensor]: ...
def _native_batch_norm_legit_no_training(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    running_mean: Tensor,
    running_var: Tensor,
    momentum: _float,
    eps: _float,
) -> tuple[Tensor, Tensor, Tensor]: ...
def _native_multi_head_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    embed_dim: _int,
    num_head: _int,
    qkv_weight: Tensor,
    qkv_bias: Tensor,
    proj_weight: Tensor,
    proj_bias: Tensor,
    mask: Tensor | None = None,
    need_weights: _bool = True,
    average_attn_weights: _bool = True,
    mask_type: _int | None = None,
) -> tuple[Tensor, Tensor]: ...
def _neg_view(input: Tensor) -> Tensor: ...
def _neg_view_copy(input: Tensor, *, out: Tensor | None = None) -> Tensor: ...
def _nested_compute_contiguous_strides_offsets(
    nested_size: Tensor,
) -> tuple[Tensor, Tensor]: ...
def _nested_from_padded(
    padded: Tensor,
    cpu_nested_shape_example: Tensor,
    fuse_transform_0213: _bool = False,
) -> Tensor: ...
def _nested_from_padded_and_nested_example(
    padded: Tensor,
    nt_example: Tensor,
) -> Tensor: ...
def _nested_from_padded_tensor(
    padded: Tensor,
    offsets: Tensor,
    dummy: Tensor,
    ragged_idx: _int = 1,
    min_seqlen: Tensor | None = None,
    max_seqlen: Tensor | None = None,
    sum_S: _int | SymInt | None = None,
) -> Tensor: ...
def _nested_get_jagged_dummy(any: Tensor) -> Tensor: ...
def _nested_get_lengths(input: Tensor) -> Tensor: ...
def _nested_get_max_seqlen(input: Tensor) -> Tensor: ...
def _nested_get_min_seqlen(input: Tensor) -> Tensor: ...
def _nested_get_offsets(input: Tensor) -> Tensor: ...
def _nested_get_ragged_idx(input: Tensor) -> _int: ...
def _nested_get_values(input: Tensor) -> Tensor: ...
def _nested_get_values_copy(
    input: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _nested_tensor_from_mask(
    t: Tensor,
    mask: Tensor,
    mask_check: _bool = True,
) -> Tensor: ...
def _nested_tensor_from_mask_left_aligned(t: Tensor, mask: Tensor) -> _bool: ...
def _nested_tensor_from_tensor_list(
    list: tuple[Tensor, ...] | list[Tensor] | None,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = None,
) -> Tensor: ...
def _nested_tensor_softmax_with_shape(
    input: Tensor,
    query: Tensor,
) -> Tensor: ...
def _nested_view_from_buffer(
    input: Tensor,
    nested_size: Tensor,
    nested_strides: Tensor,
    offsets: Tensor,
) -> Tensor: ...
def _nested_view_from_buffer_copy(
    input: Tensor,
    nested_size: Tensor,
    nested_strides: Tensor,
    offsets: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _nested_view_from_jagged(
    input: Tensor,
    offsets: Tensor,
    dummy: Tensor,
    lengths: Tensor | None = None,
    ragged_idx: _int = 1,
    min_seqlen: Tensor | None = None,
    max_seqlen: Tensor | None = None,
) -> Tensor: ...
def _nested_view_from_jagged_copy(
    input: Tensor,
    offsets: Tensor,
    dummy: Tensor,
    lengths: Tensor | None = None,
    ragged_idx: _int = 1,
    min_seqlen: Tensor | None = None,
    max_seqlen: Tensor | None = None,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _nnpack_available() -> _bool: ...
def _nnpack_spatial_convolution(
    input: Tensor,
    weight: Tensor,
    bias: Tensor | None,
    padding: _int | SymInt | Sequence[_int | SymInt],
    stride: _int | SymInt | Sequence[_int | SymInt] = 1,
) -> Tensor: ...
def _pack_padded_sequence(
    input: Tensor,
    lengths: Tensor,
    batch_first: _bool,
) -> tuple[Tensor, Tensor]: ...
def _pad_packed_sequence(
    data: Tensor,
    batch_sizes: Tensor,
    batch_first: _bool,
    padding_value: Number | _complex,
    total_length: _int,
) -> tuple[Tensor, Tensor]: ...
def _pin_memory(
    input: Tensor,
    device: DeviceLikeType | None = None,
) -> Tensor: ...
def _prelu_kernel(input: Tensor, weight: Tensor) -> Tensor: ...
def _print(s: str) -> None: ...
def _propagate_xla_data(input: Tensor, output: Tensor) -> None: ...
def _remove_batch_dim(
    input: Tensor,
    level: _int,
    batch_size: _int | SymInt,
    out_dim: _int,
) -> Tensor: ...
def _reshape_alias_copy(
    input: Tensor,
    size: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _reshape_from_tensor(input: Tensor, shape: Tensor) -> Tensor: ...
def _resize_output_(
    input: Tensor,
    size: Sequence[_int | SymInt],
    device: DeviceLikeType | None,
) -> Tensor: ...
def _rowwise_prune(
    weight: Tensor,
    mask: Tensor,
    compressed_indices_dtype: _dtype,
) -> tuple[Tensor, Tensor]: ...
def _safe_softmax(
    input: Tensor,
    dim: _int,
    dtype: _dtype | None = None,
) -> Tensor: ...
def _sample_dirichlet(
    input: Tensor,
    generator: Generator | None = None,
) -> Tensor: ...
def _saturate_weight_to_fp16(weight: Tensor) -> Tensor: ...
def _scaled_dot_product_attention_math(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attn_mask: Tensor | None = None,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    dropout_mask: Tensor | None = None,
    *,
    scale: _float | None = None,
    enable_gqa: _bool = False,
) -> tuple[Tensor, Tensor]: ...
def _scaled_dot_product_attention_math_for_mps(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attn_mask: Tensor | None = None,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    dropout_mask: Tensor | None = None,
    *,
    scale: _float | None = None,
) -> tuple[Tensor, Tensor]: ...
def _scaled_dot_product_cudnn_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attn_bias: Tensor | None,
    compute_log_sumexp: _bool,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    return_debug_mask: _bool = False,
    *,
    scale: _float | None = None,
) -> torch.return_types._scaled_dot_product_cudnn_attention: ...
def _scaled_dot_product_efficient_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attn_bias: Tensor | None,
    compute_log_sumexp: _bool,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    *,
    scale: _float | None = None,
) -> torch.return_types._scaled_dot_product_efficient_attention: ...
def _scaled_dot_product_flash_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    return_debug_mask: _bool = False,
    *,
    scale: _float | None = None,
) -> torch.return_types._scaled_dot_product_flash_attention: ...
def _scaled_dot_product_flash_attention_for_cpu(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    dropout_p: _float = 0.0,
    is_causal: _bool = False,
    *,
    attn_mask: Tensor | None = None,
    scale: _float | None = None,
) -> torch.return_types._scaled_dot_product_flash_attention_for_cpu: ...
def _scaled_grouped_mm(
    input: Tensor,
    mat2: Tensor,
    scale_a: Tensor,
    scale_b: Tensor,
    offs: Tensor | None = None,
    bias: Tensor | None = None,
    scale_result: Tensor | None = None,
    out_dtype: _dtype | None = None,
    use_fast_accum: _bool = False,
) -> Tensor: ...
def _scaled_mm(
    input: Tensor,
    mat2: Tensor,
    scale_a: Tensor,
    scale_b: Tensor,
    bias: Tensor | None = None,
    scale_result: Tensor | None = None,
    out_dtype: _dtype | None = None,
    use_fast_accum: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _shape_as_tensor(input: Tensor) -> Tensor: ...
def _sobol_engine_draw(
    quasi: Tensor,
    n: _int,
    sobolstate: Tensor,
    dimension: _int,
    num_generated: _int,
    dtype: _dtype | None,
) -> tuple[Tensor, Tensor]: ...
def _sobol_engine_ff_(
    input: Tensor,
    n: _int,
    sobolstate: Tensor,
    dimension: _int,
    num_generated: _int,
) -> Tensor: ...
def _sobol_engine_initialize_state_(
    input: Tensor,
    dimension: _int,
) -> Tensor: ...
def _sobol_engine_scramble_(
    input: Tensor,
    ltm: Tensor,
    dimension: _int,
) -> Tensor: ...
def _softmax(
    input: Tensor,
    dim: _int,
    half_to_float: _bool,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _softmax_backward_data(
    grad_output: Tensor,
    output: Tensor,
    dim: _int,
    input_dtype: _dtype,
    *,
    grad_input: Tensor | None = None,
) -> Tensor: ...
def _sparse_broadcast_to(input: Tensor, size: _size) -> Tensor: ...
def _sparse_broadcast_to_copy(
    input: Tensor,
    size: _size,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _sparse_csr_prod(
    input: Tensor,
    dim: _int | _size,
    keepdim: _bool = False,
    *,
    dtype: _dtype | None = None,
) -> Tensor: ...
def _sparse_csr_sum(
    input: Tensor,
    dim: _int | _size,
    keepdim: _bool = False,
    *,
    dtype: _dtype | None = None,
) -> Tensor: ...
def _sparse_log_softmax_backward_data(
    grad_output: Tensor,
    output: Tensor,
    dim: _int,
    input: Tensor,
) -> Tensor: ...
def _sparse_semi_structured_addmm(
    input: Tensor,
    mat1: Tensor,
    mat1_meta: Tensor,
    mat2: Tensor,
    *,
    alpha: Number | _complex = 1,
    beta: Number | _complex = 1,
    out_dtype: _dtype | None = None,
) -> Tensor: ...
def _sparse_semi_structured_apply(
    input: Tensor,
    thread_masks: Tensor,
) -> tuple[Tensor, Tensor]: ...
def _sparse_semi_structured_apply_dense(
    input: Tensor,
    thread_masks: Tensor,
) -> Tensor: ...
def _sparse_semi_structured_linear(
    input: Tensor,
    weight: Tensor,
    meta: Tensor,
    *,
    bias: Tensor | None = None,
    activation: str | None = None,
    out_dtype: _dtype | None = None,
) -> Tensor: ...
def _sparse_semi_structured_mm(
    mat1: Tensor,
    mat1_meta: Tensor,
    mat2: Tensor,
    *,
    out_dtype: _dtype | None = None,
) -> Tensor: ...
def _sparse_semi_structured_tile(
    input: Tensor,
    algorithm: str = "",
    use_cutlass: _bool = True,
) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _sparse_softmax_backward_data(
    grad_output: Tensor,
    output: Tensor,
    dim: _int,
    input: Tensor,
) -> Tensor: ...
def _sparse_sparse_matmul(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def _sparse_sum(input: Tensor) -> Tensor: ...
@overload
def _sparse_sum(input: Tensor, *, dtype: _dtype) -> Tensor: ...
@overload
def _sparse_sum(input: Tensor, dim: _int | _size) -> Tensor: ...
@overload
def _sparse_sum(
    input: Tensor,
    dim: _int | _size,
    *,
    dtype: _dtype,
) -> Tensor: ...
def _stack(
    tensors: tuple[Tensor, ...] | list[Tensor] | None,
    dim: _int = 0,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _standard_gamma(
    input: Tensor,
    generator: Generator | None = None,
) -> Tensor: ...
def _standard_gamma_grad(input: Tensor, output: Tensor) -> Tensor: ...
def _sync(t: Tensor) -> None: ...
@overload
def _test_autograd_multiple_dispatch(input: Tensor) -> Tensor: ...
@overload
def _test_autograd_multiple_dispatch(input: Tensor, b: _bool) -> Tensor: ...
def _test_autograd_multiple_dispatch_view(input: Tensor) -> Tensor: ...
def _test_autograd_multiple_dispatch_view_copy(
    input: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def _test_check_tensor(input: Tensor) -> Tensor: ...
def _test_functorch_fallback(input: Tensor, other: Tensor) -> Tensor: ...
def _test_parallel_materialize(
    input: Tensor,
    num_parallel: _int,
    skip_first: _bool = False,
) -> Tensor: ...
def _test_serialization_subcmul(
    input: Tensor,
    other: Tensor,
    alpha: Number | _complex = 1,
) -> Tensor: ...
def _to_cpu(
    tensors: tuple[Tensor, ...] | list[Tensor] | None,
) -> tuple[Tensor, ...]: ...
def _to_functional_tensor(t: Tensor) -> Tensor: ...
def _to_sparse_semi_structured(dense: Tensor) -> tuple[Tensor, Tensor]: ...
def _transform_bias_rescale_qkv(
    qkv: Tensor,
    qkv_bias: Tensor,
    num_heads: _int,
) -> tuple[Tensor, Tensor, Tensor]: ...
def _transformer_encoder_layer_fwd(
    src: Tensor,
    embed_dim: _int,
    num_heads: _int,
    qkv_weight: Tensor,
    qkv_bias: Tensor,
    proj_weight: Tensor,
    proj_bias: Tensor,
    use_gelu: _bool,
    norm_first: _bool,
    eps: _float,
    norm_weight_1: Tensor,
    norm_bias_1: Tensor,
    norm_weight_2: Tensor,
    norm_bias_2: Tensor,
    ffn_weight_1: Tensor,
    ffn_bias_1: Tensor,
    ffn_weight_2: Tensor,
    ffn_bias_2: Tensor,
    mask: Tensor | None = None,
    mask_type: _int | None = None,
) -> Tensor: ...
def _trilinear(
    i1: Tensor,
    i2: Tensor,
    i3: Tensor,
    expand1: _size,
    expand2: _size,
    expand3: _size,
    sumdim: _size,
    unroll_dim: _int = 1,
) -> Tensor: ...
def _triton_multi_head_attention(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    embed_dim: _int,
    num_head: _int,
    qkv_weight: Tensor,
    qkv_bias: Tensor,
    proj_weight: Tensor,
    proj_bias: Tensor,
    mask: Tensor | None = None,
) -> Tensor: ...
def _triton_scaled_dot_attention(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    dropout_p: _float = 0.0,
) -> Tensor: ...
def _unique(
    input: Tensor,
    sorted: _bool = True,
    return_inverse: _bool = False,
) -> tuple[Tensor, Tensor]: ...
def _unique2(
    input: Tensor,
    sorted: _bool = True,
    return_inverse: _bool = False,
    return_counts: _bool = False,
) -> tuple[Tensor, Tensor, Tensor]: ...
def _unpack_dual(
    dual: Tensor,
    level: _int,
) -> torch.return_types._unpack_dual: ...
def _unsafe_index(
    input: Tensor,
    indices: tuple[Tensor, ...] | list[Tensor] | None,
) -> Tensor: ...
def _unsafe_index_put(
    input: Tensor,
    indices: tuple[Tensor, ...] | list[Tensor] | None,
    values: Tensor,
    accumulate: _bool = False,
) -> Tensor: ...
def _unsafe_masked_index(
    input: Tensor,
    mask: Tensor,
    indices: tuple[Tensor, ...] | list[Tensor] | None,
    fill: Number | _complex,
) -> Tensor: ...
def _unsafe_masked_index_put_accumulate(
    input: Tensor,
    mask: Tensor,
    indices: tuple[Tensor, ...] | list[Tensor] | None,
    values: Tensor,
) -> Tensor: ...
@overload
def _use_cudnn_ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: Tensor,
    target_lengths: Tensor,
    blank: _int,
) -> _bool: ...
@overload
def _use_cudnn_ctc_loss(
    log_probs: Tensor,
    targets: Tensor,
    input_lengths: _size,
    target_lengths: _size,
    blank: _int,
) -> _bool: ...
def _use_cudnn_rnn_flatten_weight() -> _bool: ...
def _validate_compressed_sparse_indices(
    is_crow: _bool,
    compressed_idx: Tensor,
    plain_idx: Tensor,
    cdim: _int,
    dim: _int,
    nnz: _int,
) -> None: ...
def _validate_sparse_bsc_tensor_args(
    ccol_indices: Tensor,
    row_indices: Tensor,
    values: Tensor,
    size: _size,
    check_pinning: _bool | None = None,
) -> None: ...
def _validate_sparse_bsr_tensor_args(
    crow_indices: Tensor,
    col_indices: Tensor,
    values: Tensor,
    size: _size,
    check_pinning: _bool | None = None,
) -> None: ...
def _validate_sparse_compressed_tensor_args(
    compressed_indices: Tensor,
    plain_indices: Tensor,
    values: Tensor,
    size: _size,
    layout: _layout,
    check_pinning: _bool | None = None,
) -> None: ...
def _validate_sparse_coo_tensor_args(
    indices: Tensor,
    values: Tensor,
    size: _size,
    is_coalesced: _bool | None = None,
    check_pinning: _bool | None = None,
) -> None: ...
def _validate_sparse_csc_tensor_args(
    ccol_indices: Tensor,
    row_indices: Tensor,
    values: Tensor,
    size: _size,
    check_pinning: _bool | None = None,
) -> None: ...
def _validate_sparse_csr_tensor_args(
    crow_indices: Tensor,
    col_indices: Tensor,
    values: Tensor,
    size: _size,
    check_pinning: _bool | None = None,
) -> None: ...
def _values_copy(input: Tensor, *, out: Tensor | None = None) -> Tensor: ...
def _weight_int4pack_mm(
    input: Tensor,
    mat2: Tensor,
    qGroupSize: _int,
    qScaleAndZeros: Tensor,
) -> Tensor: ...
def _weight_int4pack_mm_for_cpu(
    input: Tensor,
    mat2: Tensor,
    qGroupSize: _int,
    qScaleAndZeros: Tensor,
) -> Tensor: ...
def _weight_int4pack_mm_with_scales_and_zeros(
    input: Tensor,
    mat2: Tensor,
    qGroupSize: _int,
    qScale: Tensor,
    qZeros: Tensor,
) -> Tensor: ...
def _weight_int8pack_mm(
    input: Tensor,
    mat2: Tensor,
    scales: Tensor,
) -> Tensor: ...
def _weight_norm(v: Tensor, g: Tensor, dim: _int = 0) -> Tensor: ...
def _weight_norm_interface(
    v: Tensor,
    g: Tensor,
    dim: _int = 0,
) -> tuple[Tensor, Tensor]: ...
def _wrapped_linear_prepack(
    weight: Tensor,
    weight_scale: Tensor,
    weight_zero_point: Tensor,
    bias: Tensor,
) -> Tensor: ...
def _wrapped_quantized_linear_prepacked(
    input: Tensor,
    input_scale: Tensor,
    input_zero_point: Tensor,
    packed_weight: Tensor,
    output_scale: Tensor,
    output_zero_point: Tensor,
    out_channel: _int,
) -> Tensor: ...
def abs(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    abs(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Computes the absolute value of each element in :attr:`input`.

    .. math::
        \text{out}_{i} = |\text{input}_{i}|

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.abs(torch.tensor([-1, -2, 3]))
        tensor([ 1,  2,  3])
    """

def abs_(input: Tensor) -> Tensor: ...
def absolute(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    absolute(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.abs`
    """

def acos(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    acos(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Computes the inverse cosine of each element in :attr:`input`.

    .. math::
        \text{out}_{i} = \cos^{-1}(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
        >>> torch.acos(a)
        tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
    """

def acos_(input: Tensor) -> Tensor: ...
def acosh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    acosh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`.

    .. math::
        \text{out}_{i} = \cosh^{-1}(\text{input}_{i})

    Note:
        The domain of the inverse hyperbolic cosine is `[1, inf)` and values outside this range
        will be mapped to ``NaN``, except for `+ INF` for which the output is mapped to `+ INF`.

    Args:
        input (Tensor): the input tensor.

    Keyword arguments:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4).uniform_(1, 2)
        >>> a
        tensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])
        >>> torch.acosh(a)
        tensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])
    """

def acosh_(input: Tensor) -> Tensor: ...
def adaptive_avg_pool1d(input: Tensor, output_size: _int | _size) -> Tensor: ...
def adaptive_max_pool1d(
    input: Tensor,
    output_size: _int | _size,
) -> tuple[Tensor, Tensor]: ...
@overload
def add(
    input: Tensor | Number | _complex,
    other: Tensor | Number | _complex,
    *,
    alpha: Number | _complex | None = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    add(input, other, *, alpha=1, out=None) -> Tensor

    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.

    .. math::
        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i


    Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
    :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

    Args:
        input (Tensor): the input tensor.
        other (Tensor or Number): the tensor or number to add to :attr:`input`.

    Keyword arguments:
        alpha (Number): the multiplier for :attr:`other`.
        out (Tensor, optional): the output tensor.

    Examples::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
        >>> torch.add(a, 20)
        tensor([ 20.0202,  21.0985,  21.3506,  19.3944])

        >>> b = torch.randn(4)
        >>> b
        tensor([-0.9732, -0.3497,  0.6245,  0.4022])
        >>> c = torch.randn(4, 1)
        >>> c
        tensor([[ 0.3743],
                [-1.7724],
                [-0.5811],
                [-0.8017]])
        >>> torch.add(b, c, alpha=10)
        tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
                [-18.6971, -18.0736, -17.0994, -17.3216],
                [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
    """

@overload
def add(self: Tensor, alpha: Number | _complex, other: Tensor) -> Tensor:
    r"""
    add(input, other, *, alpha=1, out=None) -> Tensor

    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.

    .. math::
        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i


    Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
    :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

    Args:
        input (Tensor): the input tensor.
        other (Tensor or Number): the tensor or number to add to :attr:`input`.

    Keyword arguments:
        alpha (Number): the multiplier for :attr:`other`.
        out (Tensor, optional): the output tensor.

    Examples::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
        >>> torch.add(a, 20)
        tensor([ 20.0202,  21.0985,  21.3506,  19.3944])

        >>> b = torch.randn(4)
        >>> b
        tensor([-0.9732, -0.3497,  0.6245,  0.4022])
        >>> c = torch.randn(4, 1)
        >>> c
        tensor([[ 0.3743],
                [-1.7724],
                [-0.5811],
                [-0.8017]])
        >>> torch.add(b, c, alpha=10)
        tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
                [-18.6971, -18.0736, -17.0994, -17.3216],
                [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
    """

@overload
def add(
    self: Tensor,
    alpha: Number | _complex,
    other: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    add(input, other, *, alpha=1, out=None) -> Tensor

    Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.

    .. math::
        \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i


    Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
    :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

    Args:
        input (Tensor): the input tensor.
        other (Tensor or Number): the tensor or number to add to :attr:`input`.

    Keyword arguments:
        alpha (Number): the multiplier for :attr:`other`.
        out (Tensor, optional): the output tensor.

    Examples::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
        >>> torch.add(a, 20)
        tensor([ 20.0202,  21.0985,  21.3506,  19.3944])

        >>> b = torch.randn(4)
        >>> b
        tensor([-0.9732, -0.3497,  0.6245,  0.4022])
        >>> c = torch.randn(4, 1)
        >>> c
        tensor([[ 0.3743],
                [-1.7724],
                [-0.5811],
                [-0.8017]])
        >>> torch.add(b, c, alpha=10)
        tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
                [-18.6971, -18.0736, -17.0994, -17.3216],
                [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
                [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
    """

@overload
def addbmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    batch1: Tensor,
    batch2: Tensor,
) -> Tensor:
    r"""
    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices stored
    in :attr:`batch1` and :attr:`batch2`,
    with a reduced add step (all matrix multiplications get accumulated
    along the first dimension).
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
    same number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    .. math::
        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
    must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.addbmm(M, batch1, batch2)
        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    """

@overload
def addbmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    batch1: Tensor,
    batch2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices stored
    in :attr:`batch1` and :attr:`batch2`,
    with a reduced add step (all matrix multiplications get accumulated
    along the first dimension).
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
    same number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    .. math::
        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
    must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.addbmm(M, batch1, batch2)
        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    """

@overload
def addbmm(
    input: Tensor,
    batch1: Tensor,
    batch2: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices stored
    in :attr:`batch1` and :attr:`batch2`,
    with a reduced add step (all matrix multiplications get accumulated
    along the first dimension).
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
    same number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    .. math::
        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
    must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.addbmm(M, batch1, batch2)
        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    """

@overload
def addbmm(
    beta: Number | _complex,
    self: Tensor,
    batch1: Tensor,
    batch2: Tensor,
) -> Tensor:
    r"""
    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices stored
    in :attr:`batch1` and :attr:`batch2`,
    with a reduced add step (all matrix multiplications get accumulated
    along the first dimension).
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
    same number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    .. math::
        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
    must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.addbmm(M, batch1, batch2)
        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    """

@overload
def addbmm(
    beta: Number | _complex,
    self: Tensor,
    batch1: Tensor,
    batch2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices stored
    in :attr:`batch1` and :attr:`batch2`,
    with a reduced add step (all matrix multiplications get accumulated
    along the first dimension).
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
    same number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    .. math::
        out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
    must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.addbmm(M, batch1, batch2)
        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    """

@overload
def addcdiv(
    self: Tensor,
    value: Number | _complex,
    tensor1: Tensor,
    tensor2: Tensor,
) -> Tensor:
    r"""
    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,
    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`.

    .. warning::
        Integer division with addcdiv is no longer supported, and in a future
        release addcdiv will perform a true division of tensor1 and tensor2.
        The historic addcdiv behavior can be implemented as
        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)
        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs.
        The future addcdiv behavior is just the latter implementation:
        (input + value * tensor1 / tensor2), for all dtypes.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}


    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the numerator tensor
        tensor2 (Tensor): the denominator tensor

    Keyword args:
        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcdiv(t, t1, t2, value=0.1)
        tensor([[-0.2312, -3.6496,  0.1312],
                [-1.0428,  3.4292, -0.1030],
                [-0.5369, -0.9829,  0.0430]])
    """

@overload
def addcdiv(
    self: Tensor,
    value: Number | _complex,
    tensor1: Tensor,
    tensor2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,
    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`.

    .. warning::
        Integer division with addcdiv is no longer supported, and in a future
        release addcdiv will perform a true division of tensor1 and tensor2.
        The historic addcdiv behavior can be implemented as
        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)
        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs.
        The future addcdiv behavior is just the latter implementation:
        (input + value * tensor1 / tensor2), for all dtypes.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}


    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the numerator tensor
        tensor2 (Tensor): the denominator tensor

    Keyword args:
        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcdiv(t, t1, t2, value=0.1)
        tensor([[-0.2312, -3.6496,  0.1312],
                [-1.0428,  3.4292, -0.1030],
                [-0.5369, -0.9829,  0.0430]])
    """

@overload
def addcdiv(
    input: Tensor,
    tensor1: Tensor,
    tensor2: Tensor,
    *,
    value: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addcdiv(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,
    multiplies the result by the scalar :attr:`value` and adds it to :attr:`input`.

    .. warning::
        Integer division with addcdiv is no longer supported, and in a future
        release addcdiv will perform a true division of tensor1 and tensor2.
        The historic addcdiv behavior can be implemented as
        (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)
        for integer inputs and as (input + value * tensor1 / tensor2) for float inputs.
        The future addcdiv behavior is just the latter implementation:
        (input + value * tensor1 / tensor2), for all dtypes.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}


    The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the numerator tensor
        tensor2 (Tensor): the denominator tensor

    Keyword args:
        value (Number, optional): multiplier for :math:`\text{tensor1} / \text{tensor2}`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcdiv(t, t1, t2, value=0.1)
        tensor([[-0.2312, -3.6496,  0.1312],
                [-1.0428,  3.4292, -0.1030],
                [-0.5369, -0.9829,  0.0430]])
    """

@overload
def addcmul(
    self: Tensor,
    value: Number | _complex,
    tensor1: Tensor,
    tensor2: Tensor,
) -> Tensor:
    r"""
    addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise multiplication of :attr:`tensor1`
    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value`
    and adds it to :attr:`input`.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i

    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the tensor to be multiplied
        tensor2 (Tensor): the tensor to be multiplied

    Keyword args:
        value (Number, optional): multiplier for :math:`tensor1 .* tensor2`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcmul(t, t1, t2, value=0.1)
        tensor([[-0.8635, -0.6391,  1.6174],
                [-0.7617, -0.5879,  1.7388],
                [-0.8353, -0.6249,  1.6511]])
    """

@overload
def addcmul(
    self: Tensor,
    value: Number | _complex,
    tensor1: Tensor,
    tensor2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise multiplication of :attr:`tensor1`
    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value`
    and adds it to :attr:`input`.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i

    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the tensor to be multiplied
        tensor2 (Tensor): the tensor to be multiplied

    Keyword args:
        value (Number, optional): multiplier for :math:`tensor1 .* tensor2`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcmul(t, t1, t2, value=0.1)
        tensor([[-0.8635, -0.6391,  1.6174],
                [-0.7617, -0.5879,  1.7388],
                [-0.8353, -0.6249,  1.6511]])
    """

@overload
def addcmul(
    input: Tensor,
    tensor1: Tensor,
    tensor2: Tensor,
    *,
    value: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

    Performs the element-wise multiplication of :attr:`tensor1`
    by :attr:`tensor2`, multiplies the result by the scalar :attr:`value`
    and adds it to :attr:`input`.

    .. math::
        \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i

    The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
    a real number, otherwise an integer.

    Args:
        input (Tensor): the tensor to be added
        tensor1 (Tensor): the tensor to be multiplied
        tensor2 (Tensor): the tensor to be multiplied

    Keyword args:
        value (Number, optional): multiplier for :math:`tensor1 .* tensor2`
        out (Tensor, optional): the output tensor.

    Example::

        >>> t = torch.randn(1, 3)
        >>> t1 = torch.randn(3, 1)
        >>> t2 = torch.randn(1, 3)
        >>> torch.addcmul(t, t1, t2, value=0.1)
        tensor([[-0.8635, -0.6391,  1.6174],
                [-0.7617, -0.5879,  1.7388],
                [-0.8353, -0.6249,  1.6511]])
    """

@overload
def addmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    mat1: Tensor,
    mat2: Tensor,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    mat1: Tensor,
    mat2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmm(
    input: Tensor,
    mat1: Tensor,
    mat2: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmm(
    input: Tensor,
    mat1: Tensor,
    mat2: Tensor,
    out_dtype: _dtype,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmm(
    beta: Number | _complex,
    self: Tensor,
    mat1: Tensor,
    mat2: Tensor,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmm(
    beta: Number | _complex,
    self: Tensor,
    mat1: Tensor,
    mat2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addmm(input, mat1, mat2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
    The matrix :attr:`input` is added to the final result.

    If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
    :math:`(m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
    and :attr:`out` will be a :math:`(n \times p)` tensor.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
    :attr:`input` is sparse the result will have the same layout and if :attr:`out`
    is provided it must have the same layout as :attr:`input`.


    .. warning::
        Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
        or may not have autograd support. If you notice missing functionality please
        open a feature request.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): matrix to be added
        mat1 (Tensor): the first matrix to be matrix multiplied
        mat2 (Tensor): the second matrix to be matrix multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2, 3)
        >>> mat1 = torch.randn(2, 3)
        >>> mat2 = torch.randn(3, 3)
        >>> torch.addmm(M, mat1, mat2)
        tensor([[-4.8716,  1.4671, -1.3746],
                [ 0.7573, -3.9555, -2.8681]])
    """

@overload
def addmv(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    mat: Tensor,
    vec: Tensor,
) -> Tensor:
    r"""
    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix-vector product of the matrix :attr:`mat` and
    the vector :attr:`vec`.
    The vector :attr:`input` is added to the final result.

    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of
    size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and
    :attr:`out` will be 1-D tensor of size `n`.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    Args:
        input (Tensor): vector to be added
        mat (Tensor): matrix to be matrix multiplied
        vec (Tensor): vector to be matrix multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2)
        >>> mat = torch.randn(2, 3)
        >>> vec = torch.randn(3)
        >>> torch.addmv(M, mat, vec)
        tensor([-0.3768, -5.5565])
    """

@overload
def addmv(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    mat: Tensor,
    vec: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix-vector product of the matrix :attr:`mat` and
    the vector :attr:`vec`.
    The vector :attr:`input` is added to the final result.

    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of
    size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and
    :attr:`out` will be 1-D tensor of size `n`.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    Args:
        input (Tensor): vector to be added
        mat (Tensor): matrix to be matrix multiplied
        vec (Tensor): vector to be matrix multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2)
        >>> mat = torch.randn(2, 3)
        >>> vec = torch.randn(3)
        >>> torch.addmv(M, mat, vec)
        tensor([-0.3768, -5.5565])
    """

@overload
def addmv(
    input: Tensor,
    mat: Tensor,
    vec: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix-vector product of the matrix :attr:`mat` and
    the vector :attr:`vec`.
    The vector :attr:`input` is added to the final result.

    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of
    size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and
    :attr:`out` will be 1-D tensor of size `n`.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    Args:
        input (Tensor): vector to be added
        mat (Tensor): matrix to be matrix multiplied
        vec (Tensor): vector to be matrix multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2)
        >>> mat = torch.randn(2, 3)
        >>> vec = torch.randn(3)
        >>> torch.addmv(M, mat, vec)
        tensor([-0.3768, -5.5565])
    """

@overload
def addmv(
    beta: Number | _complex,
    self: Tensor,
    mat: Tensor,
    vec: Tensor,
) -> Tensor:
    r"""
    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix-vector product of the matrix :attr:`mat` and
    the vector :attr:`vec`.
    The vector :attr:`input` is added to the final result.

    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of
    size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and
    :attr:`out` will be 1-D tensor of size `n`.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    Args:
        input (Tensor): vector to be added
        mat (Tensor): matrix to be matrix multiplied
        vec (Tensor): vector to be matrix multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2)
        >>> mat = torch.randn(2, 3)
        >>> vec = torch.randn(3)
        >>> torch.addmv(M, mat, vec)
        tensor([-0.3768, -5.5565])
    """

@overload
def addmv(
    beta: Number | _complex,
    self: Tensor,
    mat: Tensor,
    vec: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a matrix-vector product of the matrix :attr:`mat` and
    the vector :attr:`vec`.
    The vector :attr:`input` is added to the final result.

    If :attr:`mat` is a :math:`(n \times m)` tensor, :attr:`vec` is a 1-D tensor of
    size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and
    :attr:`out` will be 1-D tensor of size `n`.

    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    Args:
        input (Tensor): vector to be added
        mat (Tensor): matrix to be matrix multiplied
        vec (Tensor): vector to be matrix multiplied

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(2)
        >>> mat = torch.randn(2, 3)
        >>> vec = torch.randn(3)
        >>> torch.addmv(M, mat, vec)
        tensor([-0.3768, -5.5565])
    """

@overload
def addmv_(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    mat: Tensor,
    vec: Tensor,
) -> Tensor: ...
@overload
def addmv_(
    input: Tensor,
    mat: Tensor,
    vec: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
) -> Tensor: ...
@overload
def addmv_(
    beta: Number | _complex,
    self: Tensor,
    mat: Tensor,
    vec: Tensor,
) -> Tensor: ...
@overload
def addr(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    vec1: Tensor,
    vec2: Tensor,
) -> Tensor:
    r"""
    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`
    and adds it to the matrix :attr:`input`.

    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the
    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix
    :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
    of size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size
    :math:`(n \times m)` and :attr:`out` will be a matrix of size
    :math:`(n \times m)`.

    Args:
        input (Tensor): matrix to be added
        vec1 (Tensor): the first vector of the outer product
        vec2 (Tensor): the second vector of the outer product

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> vec1 = torch.arange(1., 4.)
        >>> vec2 = torch.arange(1., 3.)
        >>> M = torch.zeros(3, 2)
        >>> torch.addr(M, vec1, vec2)
        tensor([[ 1.,  2.],
                [ 2.,  4.],
                [ 3.,  6.]])
    """

@overload
def addr(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    vec1: Tensor,
    vec2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`
    and adds it to the matrix :attr:`input`.

    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the
    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix
    :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
    of size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size
    :math:`(n \times m)` and :attr:`out` will be a matrix of size
    :math:`(n \times m)`.

    Args:
        input (Tensor): matrix to be added
        vec1 (Tensor): the first vector of the outer product
        vec2 (Tensor): the second vector of the outer product

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> vec1 = torch.arange(1., 4.)
        >>> vec2 = torch.arange(1., 3.)
        >>> M = torch.zeros(3, 2)
        >>> torch.addr(M, vec1, vec2)
        tensor([[ 1.,  2.],
                [ 2.,  4.],
                [ 3.,  6.]])
    """

@overload
def addr(
    input: Tensor,
    vec1: Tensor,
    vec2: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`
    and adds it to the matrix :attr:`input`.

    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the
    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix
    :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
    of size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size
    :math:`(n \times m)` and :attr:`out` will be a matrix of size
    :math:`(n \times m)`.

    Args:
        input (Tensor): matrix to be added
        vec1 (Tensor): the first vector of the outer product
        vec2 (Tensor): the second vector of the outer product

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> vec1 = torch.arange(1., 4.)
        >>> vec2 = torch.arange(1., 3.)
        >>> M = torch.zeros(3, 2)
        >>> torch.addr(M, vec1, vec2)
        tensor([[ 1.,  2.],
                [ 2.,  4.],
                [ 3.,  6.]])
    """

@overload
def addr(
    beta: Number | _complex,
    self: Tensor,
    vec1: Tensor,
    vec2: Tensor,
) -> Tensor:
    r"""
    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`
    and adds it to the matrix :attr:`input`.

    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the
    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix
    :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
    of size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size
    :math:`(n \times m)` and :attr:`out` will be a matrix of size
    :math:`(n \times m)`.

    Args:
        input (Tensor): matrix to be added
        vec1 (Tensor): the first vector of the outer product
        vec2 (Tensor): the second vector of the outer product

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> vec1 = torch.arange(1., 4.)
        >>> vec2 = torch.arange(1., 3.)
        >>> M = torch.zeros(3, 2)
        >>> torch.addr(M, vec1, vec2)
        tensor([[ 1.,  2.],
                [ 2.,  4.],
                [ 3.,  6.]])
    """

@overload
def addr(
    beta: Number | _complex,
    self: Tensor,
    vec1: Tensor,
    vec2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor

    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`
    and adds it to the matrix :attr:`input`.

    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the
    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix
    :attr:`input` respectively.

    .. math::
        \text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
    of size `m`, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size
    :math:`(n \times m)` and :attr:`out` will be a matrix of size
    :math:`(n \times m)`.

    Args:
        input (Tensor): matrix to be added
        vec1 (Tensor): the first vector of the outer product
        vec2 (Tensor): the second vector of the outer product

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{vec1} \otimes \text{vec2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> vec1 = torch.arange(1., 4.)
        >>> vec2 = torch.arange(1., 3.)
        >>> M = torch.zeros(3, 2)
        >>> torch.addr(M, vec1, vec2)
        tensor([[ 1.,  2.],
                [ 2.,  4.],
                [ 3.,  6.]])
    """

def adjoint(input: Tensor) -> Tensor:
    r"""
    adjoint(input: Tensor) -> Tensor
    Returns a view of the tensor conjugated and with the last two dimensions transposed.

    ``x.adjoint()`` is equivalent to ``x.transpose(-2, -1).conj()`` for complex tensors and
    to ``x.transpose(-2, -1)`` for real tensors.

    Args:
        {input}

    Example::

        >>> x = torch.arange(4, dtype=torch.float)
        >>> A = torch.complex(x, x).reshape(2, 2)
        >>> A
        tensor([[0.+0.j, 1.+1.j],
                [2.+2.j, 3.+3.j]])
        >>> A.adjoint()
        tensor([[0.-0.j, 2.-2.j],
                [1.-1.j, 3.-3.j]])
        >>> (A.adjoint() == A.mH).all()
        tensor(True)
    """

def affine_grid_generator(
    theta: Tensor,
    size: Sequence[_int | SymInt],
    align_corners: _bool,
) -> Tensor: ...
def alias_copy(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    Performs the same operation as :func:`torch.alias`, but all output tensors
    are freshly created instead of aliasing the input.
    """

@overload
def all(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    all(input: Tensor, *, out=None) -> Tensor

    Tests if all elements in :attr:`input` evaluate to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.all(a)
        tensor(False, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.all(a)
        tensor(False)

    .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if all elements in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(4, 2).bool()
        >>> a
        tensor([[True, True],
                [True, False],
                [True, True],
                [True, True]], dtype=torch.bool)
        >>> torch.all(a, dim=1)
        tensor([ True, False,  True,  True], dtype=torch.bool)
        >>> torch.all(a, dim=0)
        tensor([ True, False], dtype=torch.bool)
    """

@overload
def all(
    input: Tensor,
    dim: _size | None = None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    all(input: Tensor, *, out=None) -> Tensor

    Tests if all elements in :attr:`input` evaluate to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.all(a)
        tensor(False, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.all(a)
        tensor(False)

    .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if all elements in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(4, 2).bool()
        >>> a
        tensor([[True, True],
                [True, False],
                [True, True],
                [True, True]], dtype=torch.bool)
        >>> torch.all(a, dim=1)
        tensor([ True, False,  True,  True], dtype=torch.bool)
        >>> torch.all(a, dim=0)
        tensor([ True, False], dtype=torch.bool)
    """

@overload
def all(
    input: Tensor,
    dim: _int,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    all(input: Tensor, *, out=None) -> Tensor

    Tests if all elements in :attr:`input` evaluate to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.all(a)
        tensor(False, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.all(a)
        tensor(False)

    .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if all elements in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(4, 2).bool()
        >>> a
        tensor([[True, True],
                [True, False],
                [True, True],
                [True, True]], dtype=torch.bool)
        >>> torch.all(a, dim=1)
        tensor([ True, False,  True,  True], dtype=torch.bool)
        >>> torch.all(a, dim=0)
        tensor([ True, False], dtype=torch.bool)
    """

@overload
def all(
    input: Tensor,
    dim: str | EllipsisType | None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    all(input: Tensor, *, out=None) -> Tensor

    Tests if all elements in :attr:`input` evaluate to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.all(a)
        tensor(False, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.all(a)
        tensor(False)

    .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if all elements in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(4, 2).bool()
        >>> a
        tensor([[True, True],
                [True, False],
                [True, True],
                [True, True]], dtype=torch.bool)
        >>> torch.all(a, dim=1)
        tensor([ True, False,  True,  True], dtype=torch.bool)
        >>> torch.all(a, dim=0)
        tensor([ True, False], dtype=torch.bool)
    """

def allclose(
    input: Tensor,
    other: Tensor,
    rtol: _float = 1e-05,
    atol: _float = 1e-08,
    equal_nan: _bool = False,
) -> _bool:
    r"""
    allclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool

    This function checks if :attr:`input` and :attr:`other` satisfy the condition:

    .. math::
        \lvert \text{input}_i - \text{other}_i \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other}_i \rvert

    elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to
    `numpy.allclose <https://numpy.org/doc/stable/reference/generated/numpy.allclose.html>`_

    Args:
        input (Tensor): first tensor to compare
        other (Tensor): second tensor to compare
        atol (float, optional): absolute tolerance. Default: 1e-08
        rtol (float, optional): relative tolerance. Default: 1e-05
        equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False``

    Example::

        >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))
        False
        >>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))
        True
        >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))
        False
        >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
        True
    """

def alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def alpha_dropout_(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def amax(
    input: Tensor,
    dim: _int | _size = (),
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    amax(input, dim, keepdim=False, *, out=None) -> Tensor

    Returns the maximum value of each slice of the :attr:`input` tensor in the given
    dimension(s) :attr:`dim`.

    .. note::
        The difference between ``max``/``min`` and ``amax``/``amin`` is:
            - ``amax``/``amin`` supports reducing on multiple dimensions,
            - ``amax``/``amin`` does not return indices.

        Both ``amax``/``amin`` evenly distribute gradients between equal values
        when there are multiple input elements with the same minimum or maximum value.

        For ``max``/``min``:
            - If reduce over all dimensions(no dim specified), gradients evenly distribute between equally ``max``/``min`` values.
            - If reduce over one specified axis, only propagate to the indexed element.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
      out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.8177,  1.4878, -0.2491,  0.9130],
                [-0.7158,  1.1775,  2.0992,  0.4817],
                [-0.0053,  0.0164, -1.3738, -0.0507],
                [ 1.9700,  1.1106, -1.0318, -1.0816]])
        >>> torch.amax(a, 1)
        tensor([1.4878, 2.0992, 0.0164, 1.9700])
    """

def amin(
    input: Tensor,
    dim: _int | _size = (),
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    amin(input, dim, keepdim=False, *, out=None) -> Tensor

    Returns the minimum value of each slice of the :attr:`input` tensor in the given
    dimension(s) :attr:`dim`.

    .. note::
        The difference between ``max``/``min`` and ``amax``/``amin`` is:
            - ``amax``/``amin`` supports reducing on multiple dimensions,
            - ``amax``/``amin`` does not return indices.

        Both ``amax``/``amin`` evenly distribute gradients between equal values
        when there are multiple input elements with the same minimum or maximum value.

        For ``max``/``min``:
            - If reduce over all dimensions(no dim specified), gradients evenly distribute between equally ``max``/``min`` values.
            - If reduce over one specified axis, only propagate to the indexed element.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
      out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.6451, -0.4866,  0.2987, -1.3312],
                [-0.5744,  1.2980,  1.8397, -0.2713],
                [ 0.9128,  0.9214, -1.7268, -0.2995],
                [ 0.9023,  0.4853,  0.9075, -1.6165]])
        >>> torch.amin(a, 1)
        tensor([-1.3312, -0.5744, -1.7268, -1.6165])
    """

def aminmax(
    input: Tensor,
    *,
    dim: _int | None = None,
    keepdim: _bool = False,
    out: Tensor | tuple[Tensor, ...] | list[Tensor] | None = None,
) -> torch.return_types.aminmax:
    r"""
    aminmax(input, *, dim=None, keepdim=False, out=None) -> (Tensor min, Tensor max)

    Computes the minimum and maximum values of the :attr:`input` tensor.

    Args:
        input (Tensor):
            The input tensor

    Keyword Args:
        dim (Optional[int]):
            The dimension along which to compute the values. If `None`,
            computes the values over the entire :attr:`input` tensor.
            Default is `None`.
        keepdim (bool):
            If `True`, the reduced dimensions will be kept in the output
            tensor as dimensions with size 1 for broadcasting, otherwise
            they will be removed, as if calling (:func:`torch.squeeze`).
            Default is `False`.
        out (Optional[Tuple[Tensor, Tensor]]):
            Optional tensors on which to write the result. Must have the same
            shape and dtype as the expected output.
            Default is `None`.

    Returns:
        A named tuple `(min, max)` containing the minimum and maximum values.

    Raises:
        RuntimeError
            If any of the dimensions to compute the values over has size 0.

    .. note::
        NaN values are propagated to the output if at least one value is NaN.

    .. seealso::
        :func:`torch.amin` computes just the minimum value
        :func:`torch.amax` computes just the maximum value

    Example::

        >>> torch.aminmax(torch.tensor([1, -3, 5]))
        torch.return_types.aminmax(
        min=tensor(-3),
        max=tensor(5))

        >>> # aminmax propagates NaNs
        >>> torch.aminmax(torch.tensor([1, -3, 5, torch.nan]))
        torch.return_types.aminmax(
        min=tensor(nan),
        max=tensor(nan))

        >>> t = torch.arange(10).view(2, 5)
        >>> t
        tensor([[0, 1, 2, 3, 4],
                [5, 6, 7, 8, 9]])
        >>> t.aminmax(dim=0, keepdim=True)
        torch.return_types.aminmax(
        min=tensor([[0, 1, 2, 3, 4]]),
        max=tensor([[5, 6, 7, 8, 9]]))
    """

def angle(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    angle(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Computes the element-wise angle (in radians) of the given :attr:`input` tensor.

    .. math::
        \text{out}_{i} = angle(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    .. note:: Starting in PyTorch 1.8, angle returns pi for negative real numbers,
              zero for non-negative real numbers, and propagates NaNs. Previously
              the function would return zero for all real numbers and not propagate
              floating-point NaNs.

    Example::

        >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159
        tensor([ 135.,  135,  -45])
    """

@overload
def any(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    any(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Tests if any element in :attr:`input` evaluates to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.any(a)
        tensor(True, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.any(a)
        tensor(True)

    .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if any element in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 2) < 0
        >>> a
        tensor([[ True,  True],
                [False,  True],
                [ True,  True],
                [False, False]])
        >>> torch.any(a, 1)
        tensor([ True,  True,  True, False])
        >>> torch.any(a, 0)
        tensor([True, True])
    """

@overload
def any(
    input: Tensor,
    dim: _size | None = None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    any(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Tests if any element in :attr:`input` evaluates to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.any(a)
        tensor(True, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.any(a)
        tensor(True)

    .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if any element in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 2) < 0
        >>> a
        tensor([[ True,  True],
                [False,  True],
                [ True,  True],
                [False, False]])
        >>> torch.any(a, 1)
        tensor([ True,  True,  True, False])
        >>> torch.any(a, 0)
        tensor([True, True])
    """

@overload
def any(
    input: Tensor,
    dim: _int,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    any(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Tests if any element in :attr:`input` evaluates to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.any(a)
        tensor(True, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.any(a)
        tensor(True)

    .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if any element in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 2) < 0
        >>> a
        tensor([[ True,  True],
                [False,  True],
                [ True,  True],
                [False, False]])
        >>> torch.any(a, 1)
        tensor([ True,  True,  True, False])
        >>> torch.any(a, 0)
        tensor([True, True])
    """

@overload
def any(
    input: Tensor,
    dim: str | EllipsisType | None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    any(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Tests if any element in :attr:`input` evaluates to `True`.

    .. note:: This function matches the behaviour of NumPy in returning
              output of dtype `bool` for all supported dtypes except `uint8`.
              For `uint8` the dtype of output is `uint8` itself.

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.rand(1, 2).bool()
        >>> a
        tensor([[False, True]], dtype=torch.bool)
        >>> torch.any(a)
        tensor(True, dtype=torch.bool)
        >>> a = torch.arange(0, 3)
        >>> a
        tensor([0, 1, 2])
        >>> torch.any(a)
        tensor(True)

    .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
       :noindex:

    For each row of :attr:`input` in the given dimension :attr:`dim`,
    returns `True` if any element in the row evaluate to `True` and `False` otherwise.


    If :attr:`keepdim` is ``True``, the output tensor is of the same size
    as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
    output tensor having 1 (or ``len(dim)``) fewer dimension(s).


    Args:
        input (Tensor): the input tensor.

        dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
            If ``None``, all dimensions are reduced.


        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4, 2) < 0
        >>> a
        tensor([[ True,  True],
                [False,  True],
                [ True,  True],
                [False, False]])
        >>> torch.any(a, 1)
        tensor([ True,  True,  True, False])
        >>> torch.any(a, 0)
        tensor([True, True])
    """

@overload
def arange(
    start: Number,
    end: Number,
    step: Number,
    *,
    out: Tensor | None = None,
    dtype: _dtype | None = None,
    device: DeviceLikeType | None = None,
    requires_grad: _bool = False,
    pin_memory: _bool = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

@overload
def arange(
    start: Number,
    end: Number,
    *,
    out: Tensor | None = None,
    dtype: _dtype | None = None,
    device: DeviceLikeType | None = None,
    requires_grad: _bool = False,
    pin_memory: _bool = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

@overload
def arange(
    end: Number,
    *,
    out: Tensor | None = None,
    dtype: _dtype | None = None,
    device: DeviceLikeType | None = None,
    requires_grad: _bool = False,
    pin_memory: _bool = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

@overload
def arange(
    end: Number | _complex,
    *,
    out: Tensor | None = None,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

@overload
def arange(
    start: Number | _complex,
    end: Number | _complex,
    *,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

@overload
def arange(
    start: Number | _complex,
    end: Number | _complex,
    step: Number | _complex = 1,
    *,
    out: Tensor | None = None,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor:
    r"""
    arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
    with values from the interval ``[start, end)`` taken with common difference
    :attr:`step` beginning from `start`.

    Note: When using floating-point dtypes (especially reduced precision types like ``bfloat16``),
    the results may be affected by floating-point rounding behavior. Some values in the sequence
    might not be exactly representable in certain floating-point formats, which can lead to
    repeated values or unexpected rounding. For precise sequences, it is recommended to use
    integer dtypes instead of floating-point dtypes.

    Note that non-integer :attr:`step` is subject to floating point rounding errors when
    comparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`
    in such cases.

    .. math::
        \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

    Args:
        start (Number, optional): the starting value for the set of points. Default: ``0``.
        end (Number): the ending value for the set of points
        step (Number, optional): the gap between each pair of adjacent points. Default: ``1``.

    Keyword args:
        out (Tensor, optional): the output tensor.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input
            arguments. If any of `start`, `end`, or `stop` are floating-point, the
            `dtype` is inferred to be the default dtype, see
            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
            be `torch.int64`.
        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
            Default: ``torch.strided``.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Example::

        >>> torch.arange(5)
        tensor([ 0,  1,  2,  3,  4])
        >>> torch.arange(1, 4)
        tensor([ 1,  2,  3])
        >>> torch.arange(1, 2.5, 0.5)
        tensor([ 1.0000,  1.5000,  2.0000])
    """

def arccos(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arccos(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.acos`.
    """

def arccos_(input: Tensor) -> Tensor: ...
def arccosh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arccosh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.acosh`.
    """

def arccosh_(input: Tensor) -> Tensor: ...
def arcsin(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arcsin(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.asin`.
    """

def arcsin_(input: Tensor) -> Tensor: ...
def arcsinh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arcsinh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.asinh`.
    """

def arcsinh_(input: Tensor) -> Tensor: ...
def arctan(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arctan(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.atan`.
    """

def arctan2(
    input: Tensor,
    other: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    arctan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -> Tensor
    Alias for :func:`torch.atan2`.
    """

def arctan_(input: Tensor) -> Tensor: ...
def arctanh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    arctanh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Alias for :func:`torch.atanh`.
    """

def arctanh_(input: Tensor) -> Tensor: ...
def argmax(
    input: Tensor,
    dim: _int | None = None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    argmax(input) -> LongTensor

    Returns the indices of the maximum value of all elements in the :attr:`input` tensor.

    This is the second value returned by :meth:`torch.max`. See its
    documentation for the exact semantics of this method.

    .. note:: If there are multiple maximal values then the indices of the first maximal value are returned.

    Args:
        input (Tensor): the input tensor.

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                [-0.7401, -0.8805, -0.3402, -1.1936],
                [ 0.4907, -1.3948, -1.0691, -0.3132],
                [-1.6092,  0.5419, -0.2993,  0.3195]])
        >>> torch.argmax(a)
        tensor(0)

    .. function:: argmax(input, dim, keepdim=False) -> LongTensor
       :noindex:

    Returns the indices of the maximum values of a tensor across a dimension.

    This is the second value returned by :meth:`torch.max`. See its
    documentation for the exact semantics of this method.

    Args:
        input (Tensor): the input tensor.

        dim (int, optional): the dimension to reduce.
     If ``None``, the argmax of the flattened input is returned.

        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                [-0.7401, -0.8805, -0.3402, -1.1936],
                [ 0.4907, -1.3948, -1.0691, -0.3132],
                [-1.6092,  0.5419, -0.2993,  0.3195]])
        >>> torch.argmax(a, dim=1)
        tensor([ 0,  2,  0,  1])
    """

def argmin(
    input: Tensor,
    dim: _int | None = None,
    keepdim: _bool = False,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    argmin(input, dim=None, keepdim=False) -> LongTensor

    Returns the indices of the minimum value(s) of the flattened tensor or along a dimension

    This is the second value returned by :meth:`torch.min`. See its
    documentation for the exact semantics of this method.

    .. note:: If there are multiple minimal values then the indices of the first minimal value are returned.

    Args:
        input (Tensor): the input tensor.

        dim (int, optional): the dimension to reduce.
     If ``None``, the argmin of the flattened input is returned.

        keepdim (bool, optional): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.


    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
                [ 1.0100, -1.1975, -0.0102, -0.4732],
                [-0.9240,  0.1207, -0.7506, -1.0213],
                [ 1.7809, -1.2960,  0.9384,  0.1438]])
        >>> torch.argmin(a)
        tensor(13)
        >>> torch.argmin(a, dim=1)
        tensor([ 2,  1,  3,  1])
        >>> torch.argmin(a, dim=1, keepdim=True)
        tensor([[2],
                [1],
                [3],
                [1]])
    """

@overload
def argsort(
    input: Tensor,
    *,
    stable: _bool,
    dim: _int = -1,
    descending: _bool = False,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    argsort(input, dim=-1, descending=False, *, stable=False) -> Tensor

    Returns the indices that sort a tensor along a given dimension in ascending
    order by value.

    This is the second value returned by :meth:`torch.sort`.  See its documentation
    for the exact semantics of this method.

    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving
    the order of equivalent elements. If ``False``, the relative order of values
    which compare equal is not guaranteed. ``True`` is slower.

    Args:
        input (Tensor): the input tensor.
        dim (int, optional): the dimension to sort along
        descending (bool, optional): controls the sorting order (ascending or descending)

    Keyword args:
        stable (bool, optional): controls the relative order of equivalent elements

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
                [ 0.1598,  0.0788, -0.0745, -1.2700],
                [ 1.2208,  1.0722, -0.7064,  1.2564],
                [ 0.0669, -0.2318, -0.8229, -0.9280]])


        >>> torch.argsort(a, dim=1)
        tensor([[2, 0, 3, 1],
                [3, 2, 1, 0],
                [2, 1, 0, 3],
                [3, 2, 1, 0]])
    """

@overload
def argsort(
    input: Tensor,
    dim: _int = -1,
    descending: _bool = False,
) -> Tensor:
    r"""
    argsort(input, dim=-1, descending=False, *, stable=False) -> Tensor

    Returns the indices that sort a tensor along a given dimension in ascending
    order by value.

    This is the second value returned by :meth:`torch.sort`.  See its documentation
    for the exact semantics of this method.

    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving
    the order of equivalent elements. If ``False``, the relative order of values
    which compare equal is not guaranteed. ``True`` is slower.

    Args:
        input (Tensor): the input tensor.
        dim (int, optional): the dimension to sort along
        descending (bool, optional): controls the sorting order (ascending or descending)

    Keyword args:
        stable (bool, optional): controls the relative order of equivalent elements

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
                [ 0.1598,  0.0788, -0.0745, -1.2700],
                [ 1.2208,  1.0722, -0.7064,  1.2564],
                [ 0.0669, -0.2318, -0.8229, -0.9280]])


        >>> torch.argsort(a, dim=1)
        tensor([[2, 0, 3, 1],
                [3, 2, 1, 0],
                [2, 1, 0, 3],
                [3, 2, 1, 0]])
    """

@overload
def argsort(
    input: Tensor,
    dim: str | EllipsisType | None,
    descending: _bool = False,
) -> Tensor:
    r"""
    argsort(input, dim=-1, descending=False, *, stable=False) -> Tensor

    Returns the indices that sort a tensor along a given dimension in ascending
    order by value.

    This is the second value returned by :meth:`torch.sort`.  See its documentation
    for the exact semantics of this method.

    If :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving
    the order of equivalent elements. If ``False``, the relative order of values
    which compare equal is not guaranteed. ``True`` is slower.

    Args:
        input (Tensor): the input tensor.
        dim (int, optional): the dimension to sort along
        descending (bool, optional): controls the sorting order (ascending or descending)

    Keyword args:
        stable (bool, optional): controls the relative order of equivalent elements

    Example::

        >>> a = torch.randn(4, 4)
        >>> a
        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],
                [ 0.1598,  0.0788, -0.0745, -1.2700],
                [ 1.2208,  1.0722, -0.7064,  1.2564],
                [ 0.0669, -0.2318, -0.8229, -0.9280]])


        >>> torch.argsort(a, dim=1)
        tensor([[2, 0, 3, 1],
                [3, 2, 1, 0],
                [2, 1, 0, 3],
                [3, 2, 1, 0]])
    """

def argwhere(input: Tensor) -> Tensor:
    r"""
    argwhere(input) -> Tensor

    Returns a tensor containing the indices of all non-zero elements of
    :attr:`input`.  Each row in the result contains the indices of a non-zero
    element in :attr:`input`. The result is sorted lexicographically, with
    the last index changing the fastest (C-style).

    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor
    :attr:`out` is of size :math:`(z \times n)`, where :math:`z` is the total number of
    non-zero elements in the :attr:`input` tensor.

    .. note::
        This function is similar to NumPy's `argwhere`.

        When :attr:`input` is on CUDA, this function causes host-device synchronization.

    Args:
        {input}

    Example::

        >>> t = torch.tensor([1, 0, 1])
        >>> torch.argwhere(t)
        tensor([[0],
                [2]])
        >>> t = torch.tensor([[1, 0, 1], [0, 1, 1]])
        >>> torch.argwhere(t)
        tensor([[0, 0],
                [0, 2],
                [1, 1],
                [1, 2]])
    """

def as_strided(
    input: Tensor,
    size: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    storage_offset: _int | SymInt | None = None,
) -> Tensor:
    r"""
    as_strided(input, size, stride, storage_offset=None) -> Tensor

    Create a view of an existing `torch.Tensor` :attr:`input` with specified
    :attr:`size`, :attr:`stride` and :attr:`storage_offset`.

    .. warning::
        Prefer using other view functions, like :meth:`torch.Tensor.view` or
        :meth:`torch.Tensor.expand`, to setting a view's strides manually with
        `as_strided`, as this function will throw an error on non-standard Pytorch
        backends (that do not have a concept of stride) and the result will depend
        on the current layout in memory. The constructed view must only refer to
        elements within the Tensor's storage or a runtime error will be thrown.
        If the generated view is "overlapped" (with multiple indices referring to
        the same element in memory), the behavior of inplace operations on this view
        is undefined (and might not throw runtime errors).

    Args:
        input (Tensor): the input tensor.
        size (tuple or ints): the shape of the output tensor
        stride (tuple or ints): the stride of the output tensor
        storage_offset (int, optional): the offset in the underlying storage of the output tensor.
            If ``None``, the storage_offset of the output tensor will match the input tensor.

    Example::

        >>> x = torch.randn(3, 3)
        >>> x
        tensor([[ 0.9039,  0.6291,  1.0795],
                [ 0.1586,  2.1939, -0.4900],
                [-0.1909, -0.7503,  1.9355]])
        >>> t = torch.as_strided(x, (2, 2), (1, 2))
        >>> t
        tensor([[0.9039, 1.0795],
                [0.6291, 0.1586]])
        >>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
        tensor([[0.6291, 0.1586],
                [1.0795, 2.1939]])
    """

def as_strided_(
    input: Tensor,
    size: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    storage_offset: _int | SymInt | None = None,
) -> Tensor: ...
def as_strided_copy(
    input: Tensor,
    size: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    storage_offset: _int | SymInt | None = None,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    Performs the same operation as :func:`torch.as_strided`, but all output tensors
    are freshly created instead of aliasing the input.
    """

def as_strided_scatter(
    input: Tensor,
    src: Tensor,
    size: Sequence[_int | SymInt],
    stride: Sequence[_int | SymInt],
    storage_offset: _int | SymInt | None = None,
) -> Tensor:
    r"""
    as_strided_scatter(input, src, size, stride, storage_offset=None) -> Tensor

    Embeds the values of the :attr:`src` tensor into :attr:`input` along
    the elements corresponding to the result of calling
    input.as_strided(size, stride, storage_offset).

    This function returns a tensor with fresh storage; it does not
    return a view.

    Args:
        input (Tensor): the input tensor.
        size (tuple or ints): the shape of the output tensor
        stride (tuple or ints): the stride of the output tensor
        storage_offset (int, optional): the offset in the underlying storage of the output tensor

    .. note::

        :attr:`src` must be of the proper size in order to be embedded
        into :attr:`input`. Specifically, it should have the same shape as
        `torch.as_strided(input, size, stride, storage_offset)`

    Example::

        >>> a = torch.arange(4).reshape(2, 2) + 1
        >>> a
        tensor([[1, 2],
                [3, 4]])
        >>> b = torch.zeros(3, 3)
        >>> b
        tensor([[0., 0., 0.],
                [0., 0., 0.],
                [0., 0., 0.]])
        >>> torch.as_strided_scatter(b, a, (2, 2), (1, 2))
        tensor([[1., 3., 2.],
                [4., 0., 0.],
                [0., 0., 0.]])
    """

def as_tensor(
    data: Any,
    dtype: _dtype | None = None,
    device: DeviceLikeType | None = None,
) -> Tensor:
    r"""
    as_tensor(data: Any, dtype: Optional[dtype] = None, device: Optional[DeviceLikeType]) -> Tensor

    Converts :attr:`data` into a tensor, sharing data and preserving autograd
    history if possible.

    If :attr:`data` is already a tensor with the requested dtype and device
    then :attr:`data` itself is returned, but if :attr:`data` is a
    tensor with a different dtype or device then it's copied as if using
    `data.to(dtype=dtype, device=device)`.

    If :attr:`data` is a NumPy array (an ndarray) with the same dtype and device then a
    tensor is constructed using :func:`torch.from_numpy`.

    If :attr:`data` is a CuPy array, the returned tensor will be located on the same device as the CuPy array unless
    specifically overwritten by :attr:`device` or a default device. The device of the CuPy array is inferred from the
    pointer of the array using `cudaPointerGetAttributes` unless :attr:`device` is provided with an explicit device index.

    .. seealso::

        :func:`torch.tensor` never shares its data and creates a new "leaf tensor" (see :doc:`/notes/autograd`).


    Args:
        data (array_like): Initial data for the tensor. Can be a list, tuple,
            NumPy ``ndarray``, scalar, and other types.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, infers data type from :attr:`data`.
        device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor
            then the device of data is used. If None and data is not a tensor then
            the result tensor is constructed on the current device.


    Example::

        >>> a = numpy.array([1, 2, 3])
        >>> t = torch.as_tensor(a)
        >>> t
        tensor([ 1,  2,  3])
        >>> t[0] = -1
        >>> a
        array([-1,  2,  3])

        >>> a = numpy.array([1, 2, 3])
        >>> t = torch.as_tensor(a, device=torch.device('cuda'))
        >>> t
        tensor([ 1,  2,  3])
        >>> t[0] = -1
        >>> a
        array([1,  2,  3])
    """

def asarray(
    obj: Any,
    *,
    dtype: _dtype | None = None,
    device: DeviceLikeType | None = None,
    copy: _bool | None = None,
    requires_grad: _bool = False,
) -> Tensor:
    r"""
    asarray(obj: Any, *, dtype: Optional[dtype], device: Optional[DeviceLikeType], copy: Optional[bool] = None, requires_grad: bool = False) -> Tensor # noqa: B950

    Converts :attr:`obj` to a tensor.

    :attr:`obj` can be one of:

    1. a tensor
    2. a NumPy array or a NumPy scalar
    3. a DLPack capsule
    4. an object that implements Python's buffer protocol
    5. a scalar
    6. a sequence of scalars

    When :attr:`obj` is a tensor, NumPy array, or DLPack capsule the returned tensor will,
    by default, not require a gradient, have the same datatype as :attr:`obj`, be on the
    same device, and share memory with it. These properties can be controlled with the
    :attr:`dtype`, :attr:`device`, :attr:`copy`, and :attr:`requires_grad` keyword arguments.
    If the returned tensor is of a different datatype, on a different device, or a copy is
    requested then it will not share its memory with :attr:`obj`. If :attr:`requires_grad`
    is ``True`` then the returned tensor will require a gradient, and if :attr:`obj` is
    also a tensor with an autograd history then the returned tensor will have the same history.

    When :attr:`obj` is not a tensor, NumPy array, or DLPack capsule but implements Python's
    buffer protocol then the buffer is interpreted as an array of bytes grouped according to
    the size of the datatype passed to the :attr:`dtype` keyword argument. (If no datatype is
    passed then the default floating point datatype is used, instead.) The returned tensor
    will have the specified datatype (or default floating point datatype if none is specified)
    and, by default, be on the CPU device and share memory with the buffer.

    When :attr:`obj` is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on
    the CPU and that doesn't share its memory (i.e. ``copy=True``). By default datatype will
    be the PyTorch datatype corresponding to the NumPy's scalar's datatype.

    When :attr:`obj` is none of the above but a scalar, or a sequence of scalars then the
    returned tensor will, by default, infer its datatype from the scalar values, be on the
    current default device, and not share its memory.

    .. seealso::

        :func:`torch.tensor` creates a tensor that always copies the data from the input object.
        :func:`torch.from_numpy` creates a tensor that always shares memory from NumPy arrays.
        :func:`torch.frombuffer` creates a tensor that always shares memory from objects that
        implement the buffer protocol.
        :func:`torch.from_dlpack` creates a tensor that always shares memory from
        DLPack capsules.

    Args:
        obj (object): a tensor, NumPy array, DLPack Capsule, object that implements Python's
               buffer protocol, scalar, or sequence of scalars.

    Keyword args:
        dtype (:class:`torch.dtype`, optional): the datatype of the returned tensor.
               Default: ``None``, which causes the datatype of the returned tensor to be
               inferred from :attr:`obj`.
        copy (bool, optional): controls whether the returned tensor shares memory with :attr:`obj`.
               Default: ``None``, which causes the returned tensor to share memory with :attr:`obj`
               whenever possible. If ``True`` then the returned tensor does not share its memory.
               If ``False`` then the returned tensor shares its memory with :attr:`obj` and an
               error is thrown if it cannot.
        device (:class:`torch.device`, optional): the device of the returned tensor.
               Default: ``None``, which causes the device of :attr:`obj` to be used. Or, if
               :attr:`obj` is a Python sequence, the current default device will be used.
        requires_grad (bool, optional): whether the returned tensor requires grad.
               Default: ``False``, which causes the returned tensor not to require a gradient.
               If ``True``, then the returned tensor will require a gradient, and if :attr:`obj`
               is also a tensor with an autograd history then the returned tensor will have
               the same history.

    Example::

        >>> a = torch.tensor([1, 2, 3])
        >>> # Shares memory with tensor 'a'
        >>> b = torch.asarray(a)
        >>> a.data_ptr() == b.data_ptr()
        True
        >>> # Forces memory copy
        >>> c = torch.asarray(a, copy=True)
        >>> a.data_ptr() == c.data_ptr()
        False

        >>> a = torch.tensor([1., 2., 3.], requires_grad=True)
        >>> b = a + 2
        >>> b
        tensor([3., 4., 5.], grad_fn=<AddBackward0>)
        >>> # Shares memory with tensor 'b', with no grad
        >>> c = torch.asarray(b)
        >>> c
        tensor([3., 4., 5.])
        >>> # Shares memory with tensor 'b', retaining autograd history
        >>> d = torch.asarray(b, requires_grad=True)
        >>> d
        tensor([3., 4., 5.], grad_fn=<AddBackward0>)

        >>> array = numpy.array([1, 2, 3])
        >>> # Shares memory with array 'array'
        >>> t1 = torch.asarray(array)
        >>> array.__array_interface__['data'][0] == t1.data_ptr()
        True
        >>> # Copies memory due to dtype mismatch
        >>> t2 = torch.asarray(array, dtype=torch.float32)
        >>> array.__array_interface__['data'][0] == t2.data_ptr()
        False

        >>> scalar = numpy.float64(0.5)
        >>> torch.asarray(scalar)
        tensor(0.5000, dtype=torch.float64)
    """

def asin(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    asin(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Returns a new tensor with the arcsine of the elements of :attr:`input`.

    .. math::
        \text{out}_{i} = \sin^{-1}(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4)
        >>> a
        tensor([-0.5962,  1.4985, -0.4396,  1.4525])
        >>> torch.asin(a)
        tensor([-0.6387,     nan, -0.4552,     nan])
    """

def asin_(input: Tensor) -> Tensor: ...
def asinh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    asinh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Returns a new tensor with the inverse hyperbolic sine of the elements of :attr:`input`.

    .. math::
        \text{out}_{i} = \sinh^{-1}(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword arguments:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])
        >>> torch.asinh(a)
        tensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])
    """

def asinh_(input: Tensor) -> Tensor: ...
def atan(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    atan(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Returns a new tensor with the arctangent of the elements of :attr:`input`.

    .. math::
        \text{out}_{i} = \tan^{-1}(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
        >>> torch.atan(a)
        tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
    """

def atan2(
    input: Tensor,
    other: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    atan2(input: Tensor, other: Tensor, *, out: Optional[Tensor]) -> Tensor

    Element-wise arctangent of :math:`\text{input}_{i} / \text{other}_{i}`
    with consideration of the quadrant. Returns a new tensor with the signed angles
    in radians between vector :math:`(\text{other}_{i}, \text{input}_{i})`
    and vector :math:`(1, 0)`. (Note that :math:`\text{other}_{i}`, the second
    parameter, is the x-coordinate, while :math:`\text{input}_{i}`, the first
    parameter, is the y-coordinate.)

    The shapes of ``input`` and ``other`` must be
    :ref:`broadcastable <broadcasting-semantics>`.

    Args:
        input (Tensor): the first input tensor
        other (Tensor): the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4)
        >>> a
        tensor([ 0.9041,  0.0196, -0.3108, -2.4423])
        >>> torch.atan2(a, torch.randn(4))
        tensor([ 0.9833,  0.0811, -1.9743, -1.4151])
    """

def atan_(input: Tensor) -> Tensor: ...
def atanh(input: Tensor, *, out: Tensor | None = None) -> Tensor:
    r"""
    atanh(input: Tensor, *, out: Optional[Tensor]) -> Tensor

    Returns a new tensor with the inverse hyperbolic tangent of the elements of :attr:`input`.

    Note:
        The domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside this range
        will be mapped to ``NaN``, except for the values `1` and `-1` for which the output is
        mapped to `+/-INF` respectively.

    .. math::
        \text{out}_{i} = \tanh^{-1}(\text{input}_{i})

    Args:
        input (Tensor): the input tensor.

    Keyword arguments:
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.randn(4).uniform_(-1, 1)
        >>> a
        tensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])
        >>> torch.atanh(a)
        tensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])
    """

def atanh_(input: Tensor) -> Tensor: ...
def avg_pool1d(
    input: Tensor,
    kernel_size: _int | _size,
    stride: _int | _size = (),
    padding: _int | _size = 0,
    ceil_mode: _bool = False,
    count_include_pad: _bool = True,
) -> Tensor: ...
@overload
def baddbmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    batch1: Tensor,
    batch2: Tensor,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def baddbmm(
    beta: Number | _complex,
    self: Tensor,
    alpha: Number | _complex,
    batch1: Tensor,
    batch2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def baddbmm(
    input: Tensor,
    batch1: Tensor,
    batch2: Tensor,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def baddbmm(
    input: Tensor,
    batch1: Tensor,
    batch2: Tensor,
    out_dtype: _dtype,
    *,
    beta: Number | _complex = 1,
    alpha: Number | _complex = 1,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def baddbmm(
    beta: Number | _complex,
    self: Tensor,
    batch1: Tensor,
    batch2: Tensor,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def baddbmm(
    beta: Number | _complex,
    self: Tensor,
    batch1: Tensor,
    batch2: Tensor,
    *,
    out: Tensor,
) -> Tensor:
    r"""
    baddbmm(input, batch1, batch2, out_dtype=None, *, beta=1, alpha=1, out=None) -> Tensor

    Performs a batch matrix-matrix product of matrices in :attr:`batch1`
    and :attr:`batch2`.
    :attr:`input` is added to the final result.

    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the same
    number of matrices.

    If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
    :math:`(b \times m \times p)` tensor, then :attr:`input` must be
    :ref:`broadcastable <broadcasting-semantics>` with a
    :math:`(b \times n \times p)` tensor and :attr:`out` will be a
    :math:`(b \times n \times p)` tensor. Both :attr:`alpha` and :attr:`beta` mean the
    same as the scaling factors used in :meth:`torch.addbmm`.

    .. math::
        \text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

    If :attr:`beta` is 0, then the content of :attr:`input` will be ignored, and `nan` and `inf` in
    it will not be propagated.

    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
    :attr:`alpha` must be real numbers, otherwise they should be integers.

    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

    Args:
        input (Tensor): the tensor to be added
        batch1 (Tensor): the first batch of matrices to be multiplied
        batch2 (Tensor): the second batch of matrices to be multiplied
        out_dtype (dtype, optional): the dtype of the output tensor,
            Supported only on CUDA and for torch.float32 given
            torch.float16/torch.bfloat16 input dtypes

    Keyword args:
        beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`\text{batch1} \mathbin{@} \text{batch2}` (:math:`\alpha`)
        out (Tensor, optional): the output tensor.

    Example::

        >>> M = torch.randn(10, 3, 5)
        >>> batch1 = torch.randn(10, 3, 4)
        >>> batch2 = torch.randn(10, 4, 5)
        >>> torch.baddbmm(M, batch1, batch2).size()
        torch.Size([10, 3, 5])
    """

@overload
def bartlett_window(
    window_length: _int,
    *,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor:
    r"""
    bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Bartlett window function.

    .. math::
        w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases}
            \frac{2n}{N - 1} & \text{if } 0 \leq n \leq \frac{N - 1}{2} \\
            2 - \frac{2n}{N - 1} & \text{if } \frac{N - 1}{2} < n < N \\
        \end{cases},

    where :math:`N` is the full window size.

    The input :attr:`window_length` is a positive integer controlling the
    returned window size. :attr:`periodic` flag determines whether the returned
    window trims off the last duplicate value from the symmetric window and is
    ready to be used as a periodic window with functions like
    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in
    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have
    ``torch.bartlett_window(L, periodic=True)`` equal to
    ``torch.bartlett_window(L + 1, periodic=False)[:-1])``.

    .. note::
        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.

    Arguments:
        window_length (int): the size of returned window
        periodic (bool, optional): If True, returns a window to be used as periodic
            function. If False, return a symmetric window.

    Keyword args:
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported.
        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only
              ``torch.strided`` (dense layout) is supported.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Returns:
        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window
    """

@overload
def bartlett_window(
    window_length: _int,
    periodic: _bool,
    *,
    dtype: _dtype | None = None,
    layout: _layout | None = None,
    device: DeviceLikeType | None = None,
    pin_memory: _bool | None = False,
    requires_grad: _bool | None = False,
) -> Tensor:
    r"""
    bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

    Bartlett window function.

    .. math::
        w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases}
            \frac{2n}{N - 1} & \text{if } 0 \leq n \leq \frac{N - 1}{2} \\
            2 - \frac{2n}{N - 1} & \text{if } \frac{N - 1}{2} < n < N \\
        \end{cases},

    where :math:`N` is the full window size.

    The input :attr:`window_length` is a positive integer controlling the
    returned window size. :attr:`periodic` flag determines whether the returned
    window trims off the last duplicate value from the symmetric window and is
    ready to be used as a periodic window with functions like
    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in
    above formula is in fact :math:`\text{window\_length} + 1`. Also, we always have
    ``torch.bartlett_window(L, periodic=True)`` equal to
    ``torch.bartlett_window(L + 1, periodic=False)[:-1])``.

    .. note::
        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.

    Arguments:
        window_length (int): the size of returned window
        periodic (bool, optional): If True, returns a window to be used as periodic
            function. If False, return a symmetric window.

    Keyword args:
        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
            Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). Only floating point types are supported.
        layout (:class:`torch.layout`, optional): the desired layout of returned window tensor. Only
              ``torch.strided`` (dense layout) is supported.
        device (:class:`torch.device`, optional): the desired device of returned tensor.
            Default: if ``None``, uses the current device for the default tensor type
            (see :func:`torch.set_default_device`). :attr:`device` will be the CPU
            for CPU tensor types and the current CUDA device for CUDA tensor types.
        requires_grad (bool, optional): If autograd should record operations on the
            returned tensor. Default: ``False``.

    Returns:
        Tensor: A 1-D tensor of size :math:`(\text{window\_length},)` containing the window
    """

def batch_norm(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    running_mean: Tensor | None,
    running_var: Tensor | None,
    training: _bool,
    momentum: _float,
    eps: _float,
    cudnn_enabled: _bool,
) -> Tensor: ...
def batch_norm_backward_elemt(
    grad_out: Tensor,
    input: Tensor,
    mean: Tensor,
    invstd: Tensor,
    weight: Tensor | None,
    sum_dy: Tensor,
    sum_dy_xmu: Tensor,
    count: Tensor,
) -> Tensor: ...
def batch_norm_backward_reduce(
    grad_out: Tensor,
    input: Tensor,
    mean: Tensor,
    invstd: Tensor,
    weight: Tensor | None,
    input_g: _bool,
    weight_g: _bool,
    bias_g: _bool,
) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
def batch_norm_elemt(
    input: Tensor,
    weight: Tensor | None,
    bias: Tensor | None,
    mean: Tensor,
    invstd: Tensor,
    eps: _float,
    *,
    out: Tensor | None = None,
) -> Tensor: ...
def batch_norm_gather_stats(
    input: Tensor,
    mean: Tensor,
    invstd: Tensor,
    running_mean: Tensor | None,
    running_var: Tensor | None,
    momentum: _float,
    eps: _float,
    count: _int,
) -> tuple[Tensor, Tensor]: ...
def batch_norm_gather_stats_with_counts(
    input: Tensor,
    mean: Tensor,
    invstd: Tensor,
    running_mean: Tensor | None,
    running_var: Tensor | None,
    momentum: _float,
    eps: _float,
    counts: Tensor,
) -> tuple[Tensor, Tensor]: ...
def batch_norm_stats(input: Tensor, eps: _float) -> tuple[Tensor, Tensor]: ...
def batch_norm_update_stats(
    input: Tensor,
    running_mean: Tensor | None,
    running_var: Tensor | None,
    momentum: _float,
) -> tuple[Tensor, Tensor]: ...
@overload
def bernoulli(
    input: Tensor,
    *,
    generator: Generator | None = None,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    bernoulli(input: Tensor, *, generator: Optional[Generator], out: Optional[Tensor]) -> Tensor

    Draws binary random numbers (0 or 1) from a Bernoulli distribution.

    The :attr:`input` tensor should be a tensor containing probabilities
    to be used for drawing the binary random number.
    Hence, all values in :attr:`input` have to be in the range:
    :math:`0 \leq \text{input}_i \leq 1`.

    The :math:`\text{i}^{th}` element of the output tensor will draw a
    value :math:`1` according to the :math:`\text{i}^{th}` probability value given
    in :attr:`input`.

    .. math::
        \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})

    The returned :attr:`out` tensor only has values 0 or 1 and is of the same
    shape as :attr:`input`.

    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating
    point ``dtype``.

    Args:
        input (Tensor): the input tensor of probability values for the Bernoulli distribution

    Keyword args:
        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
        >>> a
        tensor([[ 0.1737,  0.0950,  0.3609],
                [ 0.7148,  0.0289,  0.2676],
                [ 0.9456,  0.8937,  0.7202]])
        >>> torch.bernoulli(a)
        tensor([[ 1.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 1.,  1.,  1.]])

        >>> a = torch.ones(3, 3) # probability of drawing "1" is 1
        >>> torch.bernoulli(a)
        tensor([[ 1.,  1.,  1.],
                [ 1.,  1.,  1.],
                [ 1.,  1.,  1.]])
        >>> a = torch.zeros(3, 3) # probability of drawing "1" is 0
        >>> torch.bernoulli(a)
        tensor([[ 0.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 0.,  0.,  0.]])
    """

@overload
def bernoulli(
    input: Tensor,
    p: _float,
    *,
    generator: Generator | None = None,
) -> Tensor:
    r"""
    bernoulli(input: Tensor, *, generator: Optional[Generator], out: Optional[Tensor]) -> Tensor

    Draws binary random numbers (0 or 1) from a Bernoulli distribution.

    The :attr:`input` tensor should be a tensor containing probabilities
    to be used for drawing the binary random number.
    Hence, all values in :attr:`input` have to be in the range:
    :math:`0 \leq \text{input}_i \leq 1`.

    The :math:`\text{i}^{th}` element of the output tensor will draw a
    value :math:`1` according to the :math:`\text{i}^{th}` probability value given
    in :attr:`input`.

    .. math::
        \text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})

    The returned :attr:`out` tensor only has values 0 or 1 and is of the same
    shape as :attr:`input`.

    :attr:`out` can have integral ``dtype``, but :attr:`input` must have floating
    point ``dtype``.

    Args:
        input (Tensor): the input tensor of probability values for the Bernoulli distribution

    Keyword args:
        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
        out (Tensor, optional): the output tensor.

    Example::

        >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]
        >>> a
        tensor([[ 0.1737,  0.0950,  0.3609],
                [ 0.7148,  0.0289,  0.2676],
                [ 0.9456,  0.8937,  0.7202]])
        >>> torch.bernoulli(a)
        tensor([[ 1.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 1.,  1.,  1.]])

        >>> a = torch.ones(3, 3) # probability of drawing "1" is 1
        >>> torch.bernoulli(a)
        tensor([[ 1.,  1.,  1.],
                [ 1.,  1.,  1.],
                [ 1.,  1.,  1.]])
        >>> a = torch.zeros(3, 3) # probability of drawing "1" is 0
        >>> torch.bernoulli(a)
        tensor([[ 0.,  0.,  0.],
                [ 0.,  0.,  0.],
                [ 0.,  0.,  0.]])
    """

def bilinear(
    input1: Tensor,
    input2: Tensor,
    weight: Tensor,
    bias: Tensor | None = None,
) -> Tensor: ...
def binary_cross_entropy_with_logits(
    input: Tensor,
    target: Tensor,
    weight: Tensor | None = None,
    pos_weight: Tensor | None = None,
    reduction: _int = 1,
) -> Tensor: ...
def bincount(
    input: Tensor,
    weights: Tensor | None = None,
    minlength: _int | SymInt = 0,
) -> Tensor:
    r"""
    bincount(input, weights=None, minlength=0) -> Tensor

    Count the frequency of each value in an array of non-negative ints.

    The number of bins (size 1) is one larger than the largest value in
    :attr:`input` unless :attr:`input` is empty, in which case the result is a
    tensor of size 0. If :attr:`minlength` is specified, the number of bins is at least
    :attr:`minlength` and if :attr:`input` is empty, then the result is tensor of size
    :attr:`minlength` filled with zeros. If ``n`` is the value at position ``i``,
    ``out[n] += weights[i]`` if :attr:`weights` is specified else
    ``out[n] += 1``.

    Note:
        This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.

    Arguments:
        input (Tensor): 1-d int tensor
        weights (Tensor): optional, weight for each value in the input tensor.
            Should be of same size as input tensor.
        minlength (int): optional, minimum number of bins. Should be non-negative.

    Returns:
        output (Tensor): a tensor of shape ``Size([max(input) + 1])`` if
        :attr:`input` is non-empty, else ``Size(0)``

    Example::

        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)
        >>> weights = torch.linspace(0, 1, steps=5)
        >>> input, weights
        (tensor([4, 3, 6, 3, 4]),
         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])

        >>> torch.bincount(input)
        tensor([0, 0, 0, 2, 2, 0, 1])

        >>> input.bincount(weights)
        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])
    """

def binomial(
    count: Tensor,
    prob: Tensor,
    generator: Generator | None = None,
) -> Tensor: ...
@overload
def bitwise_and(
    input: Tensor,
    other: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    bitwise_and(input, other, *, out=None) -> Tensor

    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
    integral or Boolean types. For bool tensors, it computes the logical AND.

    Args:
        input: the first input tensor
        other: the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
        tensor([1, 0,  3], dtype=torch.int8)
        >>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
        tensor([ False, True, False])
    """

@overload
def bitwise_and(self: Number | _complex, other: Tensor) -> Tensor:
    r"""
    bitwise_and(input, other, *, out=None) -> Tensor

    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
    integral or Boolean types. For bool tensors, it computes the logical AND.

    Args:
        input: the first input tensor
        other: the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
        tensor([1, 0,  3], dtype=torch.int8)
        >>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
        tensor([ False, True, False])
    """

@overload
def bitwise_and(
    input: Tensor,
    other: Number | _complex,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    bitwise_and(input, other, *, out=None) -> Tensor

    Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
    integral or Boolean types. For bool tensors, it computes the logical AND.

    Args:
        input: the first input tensor
        other: the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
        tensor([1, 0,  3], dtype=torch.int8)
        >>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
        tensor([ False, True, False])
    """

@overload
def bitwise_left_shift(
    input: Tensor,
    other: Tensor,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    bitwise_left_shift(input, other, *, out=None) -> Tensor

    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits.
    The input tensor must be of integral type. This operator supports
    :ref:`broadcasting to a common shape <broadcasting-semantics>` and
    :ref:`type promotion <type-promotion-doc>`.

    The operation applied is:

    .. math::
        \text{out}_i = \text{input}_i << \text{other}_i

    Args:
        input (Tensor or Scalar): the first input tensor
        other (Tensor or Scalar): the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
        tensor([-2, -2, 24], dtype=torch.int8)
    """

@overload
def bitwise_left_shift(self: Number | _complex, other: Tensor) -> Tensor:
    r"""
    bitwise_left_shift(input, other, *, out=None) -> Tensor

    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits.
    The input tensor must be of integral type. This operator supports
    :ref:`broadcasting to a common shape <broadcasting-semantics>` and
    :ref:`type promotion <type-promotion-doc>`.

    The operation applied is:

    .. math::
        \text{out}_i = \text{input}_i << \text{other}_i

    Args:
        input (Tensor or Scalar): the first input tensor
        other (Tensor or Scalar): the second input tensor

    Keyword args:
        out (Tensor, optional): the output tensor.

    Example::

        >>> torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
        tensor([-2, -2, 24], dtype=torch.int8)
    """

@overload
def bitwise_left_shift(
    input: Tensor,
    other: Number | _complex,
    *,
    out: Tensor | None = None,
) -> Tensor:
    r"""
    bitwise_left_shift(input, other, *, out=None) -> Tensor

    Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits.
    The input tensor must be of integral type. This operator supports
    :ref:`broadcasting to a common shape <broadcasting-semantics>` and
    :ref:`type promotion <type-promotion-doc>`.

    The operation applied is:

    .. math::
        \text{out}_i = \text{input}_i << \text{other}_i

    Args:
        input (Tensor or Scalar): the first input tensor
        o